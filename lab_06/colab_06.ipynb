{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZBTJY39Bqi9"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"./fs_logo.png\">\n",
    "\n",
    "##  Lab 06 - Adversarial Deep Learning\n",
    "\n",
    "Seminar Künstliche Intelligenz, Frankfurt School, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSFSdKo2Bqi_"
   },
   "source": [
    "Die Analysen des Seminars **Künstliche Intelligenz** des Zertifikatstudiengangs **Certified Audit Data Scientist (CADS)** basieren auf Jupyter Notebook. Anhand solcher Notebooks ist es möglich eine Vielzahl von Datenanalysen und statistischen Validierungen durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"./lab_06_banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Lab haben Sie die verschiedenen Elemente eines Unsupervised Deep Learning Workflow kennengelernt z.B. Datenaufbereitung, Modell Training und Modell Validierung. In diesem sechsten Lab werden wir Jupyter Notebook verwenden, um ein weiteres **Deep Learning basiertes Audit-Analyseverfahren** zu implementieren und anzuwenden.\n",
    "\n",
    "Hierzu werden wir die im Seminar vorgestellten Adversarial Autoencoder Neural Networks (Ad-AENNs) anwenden um (1) zunächst die generativen Prozesse einer Finanzbuchhaltung und (2) anschliessend entsprechende Prozessanomalien zu detektieren. Im Gegensatz zu klassischen Autoencoder Netzen lernen AENNs, die Eingabedaten in eine niedrig-dimensionale Repräsentation zu **encodieren**.  Gleichzeitig lernt das AENN, die ursprünglichen Daten wieder aus der enkodierten Repräsentation zu **dekodieren**. \n",
    "\n",
    "Die dekodierten Daten, die in der Regel als **Rekonstruktion** bezeichnet werden, sollten eine grosse Ähnlichkeit zu den ursprünglichen **Eingabedaten** aufweisen. Die Buchungssätze, für welche eine erfolgreiche Rekonstruktion nur fehlerhaft gelingt müssen deshalb eine oder mehrere ungewöhnliche Eigenschaften aufweisen. Die nachstehende Abbildung zeigt einen Überblick über den Deep Learning Prozess bzw. die AENN Netzarchitektur, welche wir in diesem Lab implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"./process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Rahmen des Lab werden wir wieder einige Funktionen der `PyTorch` Bibliothek nutzen, um das AENN zu implementieren und zu trainieren. Im Laufe des Trainingsprozess soll das AENN die charakteristische Eigenschaften historischer **Buchungen** bzw. **Journal Entries** lernen. Nach erfolgreichen Modelltraining, werden wir das Modell anwenden, um anhand des Rekonstruktionsfehlers ungewöhnliche Buchungen innerhalb des Datensatzes zu detektieren. Abschliessend werden wir die gelernten **Repräsentationen** der einzelnen Journaleinträge dazu verwenden, um die erhaltenen Ergebnisse noch aussagekräftiger zu interpretieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei etwaigen Fragen wenden Sie sich, wie immer gerne an uns via **marco (dot) schreyer (at) unisg (dot) ch**. Wir wünschen Ihnen Viel Freude mit unseren Notebooks und Ihren revisorischen Analysen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lernziele des Labs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der heutigen Übung sollten Sie in der Lage sein:\n",
    "\n",
    ">1. Die **Grundkonzepte, Funktionsweise und Bestandteile** von Adversarial Autoencoder Netzen zu verstehen.\n",
    ">2. Eine **Vorverarbeitung** von kategorischen Finanzdaten (d.h. One-Hot Encoding und Min-Max Normalisierung) durchzuführen. \n",
    ">3. Adversarial Autoencoder Netze anzuwenden, um **Anomalien** in umfangreichen Finanzdaten aufzuspüren.\n",
    ">4. Die **Ergebnisse** bzw. den Latenten Raum von Adversarial Autoencoder Netzen zu interpretieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einrichten der Analyseumgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie in den vorangegangenen Übungen werden wir zunächst eine Reihe von Python-Bibliotheken importieren, welche die Datenanalyse und -visualisierung ermöglichen. In dieser Übung werden wir die Bibliotheken `PyTorch`, `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` und `Seaborn` verwenden. Nachfolgend importieren wir die benötigten Bibliotheken durch die Ausführung der folgenden Anweisungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os, sys, itertools, urllib, io, warnings\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader as dr\n",
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import der `PyTorch` Deep Learning Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import der `Matplotlib` und `Seaborn` Visualisierungs Bibliotheken und setzen der Visualisierungsparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausschalten möglicher Warnmeldungen z.B. aufgrund von zukünftigen Änderungen der Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the warning filter flag to ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktivieren der sog. Inline-Darstellung von Visualisierungen in Jupyter-Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen von Unterverzeichnissen innerhalb des aktuellen Arbeitsverzeichnisses für (1) die `GDrive` Notebooks im Allgemeinen, (2) das Speichern der Originaldaten, (3) der Analyseergebnisse und (4) der trainierten Modelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Colab Notebooks directory\n",
    "notebook_directory = '/content/drive/MyDrive/Colab Notebooks'\n",
    "if not os.path.exists(notebook_directory): os.makedirs(notebook_directory)\n",
    "\n",
    " # create data sub-directory inside the Colab Notebooks directory\n",
    "data_directory = '/content/drive/MyDrive/Colab Notebooks/01_data'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "# create results sub-directory inside the Colab Notebooks directory\n",
    "results_directory = '/content/drive/MyDrive/Colab Notebooks/02_results'\n",
    "if not os.path.exists(results_directory): os.makedirs(results_directory)\n",
    "\n",
    " # create models sub-directory inside the Colab Notebooks directory\n",
    "models_directory = '/content/drive/MyDrive/Colab Notebooks/03_models'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Festlegen eines zufälligen Seeds zur Gewährleistung der Reproduzierbarkeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktivieren des GPU Computing, durch setzen des `device` flag und setzen eines zufälligen `CUDA` Seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# set pytorch gpu seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] notebook with {} computation enabled'.format(str(now), str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzeige der Hardware Informationen zu den ggf. verfügbaren GPU(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzeige der Software Informationen über die verfügbaren `Python` bzw. `PyTorch` Versionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current Python version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The Python version: {}'.format(now, sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current PyTorch version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datenakquise und Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heutzutage beschleunigen Unternehmen die Digitalisierung von Geschäftsprozessen, wovon auch Enterprise Resource Planning (ERP)-Systeme betroffen sind. Diese Systeme sammeln grosse Mengen Daten auf granularer Ebene. Dies gilt insbesondere für die Journalbuchungen einer Organisation, die innerhalb des Hauptbuch und den jeweiligen Nebenbüchern erfasst werden.\n",
    "\n",
    "Die Darstellung in **Abbildung 1** zeigt eine hierarchische Ansicht eines ERP-Systems, das Journalbuchungen in Datenbanktabellen erfasst. Im Kontext revisorischer Prüfungen können die in solchen Systemen erfassten Daten Spuren bzw. wertvolle Hinweise auf mögliche dolose Handlungen enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"./accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abbildung 1:** Hierarchische Ansicht eines Enterprise Resource Planning (ERP)-Systems, das Geschäftsvorfälle auf verschiedene Abstraktionsebenen in Datenbanktabellen erfasst, d.h. auf Ebene (1) des Geschäftsprozesses, (2) der Buchhaltung sowie (3) der Datenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden wir den im Rahmen des Labs verwendeten Datensatzes deskriptiv analysieren. Anschliessend werden wir die Daten vorverarbeiten um eine Ausgangslage für das Training eines Neuronalen Netzes zu schaffen. Der Lab Datensatz basiert auf einer angepassten Teilmenge des **\"Synthetic Financial Dataset For Fraud Detection \"** Datensatz von Lopez-Rojas. Der Originaldatensatz wurde ursprünglich über die Kaggle-Plattform für Data Science Wettbewerbe veröffentlicht und kann über den nachfolgenden Link abgerufen werden kann: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "In einem ersten Schritt laden wir den Datensatz in unsere Analyseumgebung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into the notebook\n",
    "url = 'https://raw.githubusercontent.com/GitiHubi/CADS/main/lab_06/fraud_dataset_v2.csv'\n",
    "ori_dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend prüfen wir die Dimensionalität des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the datasets dimensionalities\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] transactional dataset of {} rows and {} columns retreived.'.format(now, ori_dataset.shape[0], ori_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darüber hinaus speichern wir eine Sicherheitskopie des geladenen Datensatzes mit aktuellem Zeitstempel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine current timestamp \n",
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# define dataset filename \n",
    "filename = timestamp + \" - original_fraud_dataset.xlsx\"\n",
    "\n",
    "# save dataset extract to the data directory\n",
    "ori_dataset.head(1000).to_excel(os.path.join(data_directory, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initiales Daten Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz enthält insgesamt **sieben kategorische** und **zwei numerische Attribute**, welche den innerhalb eines SAP FICO Moduls enthaltenen Tabellen BKPF (Buchungsbelegköpfe) und BSEG (Buchungsbelegsegmente) entsprechen. Die nachstehende Liste enthält einen Überblick über die einzelnen Attribute sowie eine kurze Beschreibung ihrer jeweiligen Semantik:\n",
    "\n",
    ">- `BELNR`: die Nummer des Buchhaltungsbelegs,\n",
    ">- `BUKRS`: der Buchungskreis\n",
    ">- `BSCHL`: der Buchungsschlüssel,\n",
    ">- `HKONT`: das gebuchte Hauptbuchkonto,\n",
    ">- `PRCTR`: das gebuchte Profit Center,\n",
    ">- `WAERS`: der Währungsschlüssel,\n",
    ">- `KTOSL`: der Schlüssel des Hauptbuchkontos,\n",
    ">- `DMBTR`: der Betrag in der Hauswährung,\n",
    ">- `WRBTR`: der Betrag in der Belegwährung.\n",
    "\n",
    "Sehen wir uns auch einmal die ersten 10 Zeilen des Datensatzes im Detail an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top rows of dataset\n",
    "ori_dataset.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vielleicht ist Ihnen bei der Durchsicht der Attribute auch das Attribut mit der Bezeichung `Label` in den Daten aufgefallen. Dieses Attribut enthält die **Ground-Truth Informationen** zu den jeweils einzelnen Buchungen. Das Attribut beschreibt die 'wahre Natur' jeder Transaktion, d.h. ob es sich um eine **reguläre** Transaktion (gekennzeichnet durch `regulär`) oder eine **Anomalie** (gekennzeichnet durch `global` und `lokal`) handelt.  \n",
    "\n",
    "Innerhalb unseres Vorgehens werden wir die Label Information nur dazu verwenden, um die Ergebnisse unserer trainierten Modelle zu validieren. Bitte beachten Sie jedoch, dass uns eine solches Feld in der Realität oftmals nicht zur Verfügung steht. Schauen wir uns nun einmal die Verteilung der regulären gegenüber den anomalen Buchungen im Datensatz an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analyse zeigt, dass wir es, ähnlich wie in der realen Welt, mit einem **unbalanzierten Datensatz** konfrontiert sind. D.h. insgesamg enthält der Datensatz nur einen sehr kleinen Anteil von **100 (0,109 %)** anomalen Transaktionen. Unter den 100 Anomalien befinden sich **70 (0,076 %)** *globale* Anomalien und **30 (0,003 %)** *lokale* Anomalien. \n",
    "\n",
    "In einem nächsten Schritt entfernen wir das `label` Attribut aus dem Trainingsdatensatz und speichern es in einer gesonderten Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the class\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vorverarbeitung der Kategorischen Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Sichtung der Daten geht hervor, dass die Mehrzahl der Attribute kategorische (diskrete) Attributwerte aufweisen, z.B. das Buchungsdatum, das Hauptbuchkonto, die Buchungsart und die Währung. Schauen wir uns nun die Verteilung der kategorischen Attribute *Buchungsschlüssel* `BSCHL` sowie *Hauptbuchkonto* `HKONT` einmal im Detail an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to plot posting key and general ledger account side by side\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "plot = sns.countplot(x=ori_dataset['BSCHL'], ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Buchungsschlüssel Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot the distribution of the general ledger attribute\n",
    "plot = sns.countplot(x=ori_dataset['HKONT'], ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Hauptbuchkonto Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Allgemeinen sind Neuronale Netze dafür konzipiert numerische Daten zu verarbeiten. Eine Möglichkeit, diese Anforderung zu erfüllen, ist die Anwendung eines Verfahrens, das als sog. **One-Hot Kodierung** bezeichnet wird. Hierdurch kann eine numerische Darstellung kategorischer Attributwerte abgeleitet werden. Bei der **One-Hot Kodierung** wird für jeden kategorischen Attributwert eine zusätzliche binäre Spalte in den Daten erstellt. \n",
    "\n",
    "Schauen wir uns hierzu das Beispiel in **Abbildung 2** unten an. Das kategorische Attribut **Receiver** in den Orginaldaten enthält die Namen 'Sally', 'John' und 'Emma'. Wir kodieren das Attribut als 'one-hot' Attribut, indem wir eine zusätzliche binäre Spalte für jeden kategorischen Wert in der Spalte 'Receiver' erstellen. Anschliessend kodieren wir z.B. jede Transaktion, die den Wert 'Sally' in der Spalte 'Receiver' aufweist mit dem Wert 1.0 innerhalb der 'Sally' Spalte der Transaktion. Sollte eine Transaktion einen anderen Wert in der Spalte 'Receiver' aufweisen, kodieren wir die 'Sally' Spalte mit dem Wert 0.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 500px; height: auto\" src=\"./encoding.png\">\n",
    "\n",
    "**Abbildung 2:** Beispielhafte 'One-Hot' Kodierung der verschiedenen Receiver Attributwerte in spezifische binäre 'One-Hot' Spalten. Dabei resultiert jeder im Datensatz beobachtbare Attributwert in einer eigene Spalte. Der Spaltenwert **1.0** kodiert das Vorkommen des Attributwertes in der entsprechenden Buchung. Der Spaltenwert **0.0** hingegen zeigt, dass der Attributwert nicht innerhalb der entsprechenden Buchung vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand dieses Verfahrens können die insgesamt sechs kategorischen Attribute des Datensatzes in numerische Attribute überführt werden. Die `Pandas` Bibliothek stellt hierzu die entsprechende Funktionalität zur Verfügung, welche wir im Nachfolgenden anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'BUKRS', 'WAERS']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_cat_processed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend überprüfen wie die vorgenommene **One-Hot Kodierung** anhand der 10 ersten Buchungen des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "ori_dataset_cat_processed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Vorverarbeitung der numerischen Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend Analysieren wir nun die Verteilungen der **beiden numerischen Attribute** des Datensatzes. Hierbei handelt es sich um die Attribute (1) *Betrag in Hauswährung* `DMBTR` und (2) *Betrag in Dokumentwährung* `WRBTR` deren jeweilge Verteilungen wir nachfolgend visualisieren: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-scaled 'DMBTR' as well as the 'WRBTR' attribute value distribution\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "plot = sns.distplot(ori_dataset['DMBTR'].tolist(), ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Hauswährung - Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "plot = sns.distplot(ori_dataset['WRBTR'].tolist(), ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Dokumentwährung - Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Werte beider Betragsattribute weisen eine jeweils **schiefe** und **steile** Verteilung auf. Wir skalieren deshalb die Werte zunächst logarithmisch. Anschliessend min-max normalisieren wir die Skalierten Werte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 'DMBTR' and 'WRBTR' attribute\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "\n",
    "# log scale the 'DMBTR' and 'WRBTR' attribute values\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_num_processed = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt visualisieren wir die Verteilungen der skalierten bzw. normierten Werte beider Betragsattribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-scaled 'DMBTR' as well as the 'WRBTR' attribute value distribution\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "plot = sns.distplot(ori_dataset_num_processed['DMBTR'].tolist(), ax=ax[0])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Hauswährung - Attribute Value Distribution', fontsize=16)\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "plot = sns.distplot(ori_dataset_num_processed['WRBTR'].tolist(), ax=ax[1])\n",
    "\n",
    "# set axis labels\n",
    "plot.set_xlabel('Attribute Value', fontsize=16)\n",
    "plot.set_ylabel('Attribute Value Count', fontsize=16)\n",
    "\n",
    "# set plot title\n",
    "plot.set_title('Betrag in Dokumentwährung - Attribute Value Distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merge Categorical and Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschliessend fügen wir die beiden vorverarbeiteten numerischen und kategorischen Attribute zu einem **einzigen Datensatz** zusammen. Der zusammengeführte Datensatz bildet die Grundlage für das nachfolgende Training des Deep Autoencoder Neural Networks (AENNs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_cat_processed, ori_dataset_num_processed], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Werfen wir nun abschliessend noch einen finalen einen Blick auf die Dimensionalität des zusammengefügten Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Abschluss der Vorverarbeitungsschritte verfügen wir über einen Datensatz, der aus einer Gesamtzahl von **533,009 Datensätzen** (Zeilen) und **618 Attributen** (Spalten) besteht. Wir behalten die Anzahl der Spalten im Hinterkopf, da sie die Dimensionalität der Eingabe- und Ausgabeschicht unseres AENNs bestimmen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQD5144JBqjs"
   },
   "source": [
    "## 3. Adversarial Autoencoder Neural Network Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt möchten wir uns mit der zugrundeliegenden Idee und dem Aufbau eines **Adversarial Autoencoder Neural Networks (AAENNs)** vertraut zu machen. Hierzu werden wir die einzelne Bausteine und die spezifische Netzwerkstruktur von AAENNs anhand der `PyTorch` Open-Source-Bibliothek implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Adversarial Autoencoder Neural Network Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vu8jws-WBqjs"
   },
   "source": [
    "Die Architektur des **Adversarial Autoencoder Neural Networks (AAENNs)**, wie in der Abbildung unten dargestellt, erweitert das Konzept klassischer Autoencoder Neural Networks (AENNs), um die Idee sog. **Generative Adversarial Networks (GANs)**. Anhand dieser Erweitung ist es dem AAENN möglich die erlernten Repräsentationen innerhalb des latenten Raum des AAENNs anhand einer beliebigen Prior-Verteilung azuordnen. Das Lernen selbst erfolgt pro Trainingsbatch in zwei Phasen, (1) einer Rekonstruktionsphase und (2) einer Regularisierungsphase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAMjNjsvBqjt"
   },
   "source": [
    "Innerhalb der **Rekonstruktionsphase** wird der AAENN-Encoder $q_{\\theta}(z|x)$ trainiert, um eine aggregierte Posterior-Verteilung $q(z)$ der Journaleinträge $X$ über den latenten Raum $Z$ zu lernen. Dabei entspricht die gelernte Posterior-Verteilung, in Analogie zu AENNs, komprimierten Repräsentatonen der Journalbuchungen. Das  Decoder-Netzwerk $p_{\\theta}(\\hat{x}|z)$ des AAENNs verwendet die erlernten Repräsentationen $z_i \\in Z$ wiederum, um die Journaleinträge so Originalgetreu wie möglich zu rekonstruieren $\\hat{X}$. **Zielsetzung der Rekonstruktionsphase ist es den Rekonstruktionsfehler der AAENNs zu minimieren**.\n",
    "\n",
    "In der **Regularisierungsphase** wird ein adversarial Trainingsverfahren angewandt bei dem das Encoder-Netzwerk $q_{\\theta}(z|x)$ zugleich als Generator-Netzwerk fungiert. Zusätzlich erfolgt der Aufsatz eines Diskriminator-Netzwerk $d_{\\theta}(z)$ auf den latenten Raum $Z$ des AAENNs. In Analogie zum Training von GANs wird das Diskriminator-Netzwerk des AAENN trainiert, um die Samples einer ausgewählten Prior-Verteilung $p(z) \\in Z$, sog. **Fake Verteilung**, von der gelernten Posterior-Verteilung $q(z)$, sog. **True Verteilung**, zu unterscheiden. **Zielsetzung der Regulierungsphase ist es das Diskriminator-Netzwerk des AAENNs maximal zu täuschen**. \n",
    "\n",
    "Eine solche Täuschung gelingt dem Generator-Netzwerk wenn sich Fake und True Verteilung $p(z) ≈ q(z)$ nicht mehr signifikant von einander unterscheiden. Das heisst, sich das Diskriminator-Netzwerk nicht sicher ist, ob ein vorliegendes Trainingssample aus der **True Verteilung** $q(z)$ oder der **Fake Verteilung** $p(z)$ stammt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BI7zJu4Bqjt"
   },
   "source": [
    "<img align=\"middle\" style=\"max-width: 830px; height: auto\" src=\"https://github.com/GitiHubi/deepAD/blob/master/images/autoencoder_2.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abbildung 3:** Schematische Darstellung eines **Adversarial Autoencoder Neural Networks (AAENNs)**, das aus drei nicht-linearen Abbildungen bzw. Feed-Forward-Netzen besteht. Die drei miteinander verknüpften Netze werden als Encoder-Netzwerk $q_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$, Decoder-Netzwerk $p_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dx}$, und Discriminator-Netzwerk $d_\\phi: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{d1}$ bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmPCygUqBqjt"
   },
   "source": [
    "### 3.1 Encoder Netzwerk Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt möchten wir nun das **Encoder Netz $q_{\\theta}(z | x)$** in `PyTorch` implementieren. Der Encoder soll aus insgesamt **fünf Schichten von fully-connected Neuronen** bestehen. Darüber hinaus solle der Encoder die nachfolgende Anzahl von Neuronen pro Schicht enthalten: 618-256-64-16-4-2. Die vorhergehende Notation bedeutet, dass die erste Schicht 618 Neuronen umfasst (bestimmt durch die Dimensionalität der Eingabedaten), die zweite Schicht 256 Neuronen und die weiteren Schichten 64, 16, 4 bzw. 2 Neuronen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hY5356SWBqjt"
   },
   "source": [
    "Den nachfolgenden drei Elementen der Implementierung des Encoder Netzes möchten wir eine besonderes Augenmerk schenken:\n",
    "\n",
    ">- `self.encoder_Lx`: definiert die lineare Transformation der jeweiligen Schicht, welche auf die Eingabe angewandt wird: $Wx + b$.\n",
    ">- `nn.init.xavier_uniform`: initialisiert Gewichtsparameter anhand einer gleichmäßigen Xavier Verteilung.\n",
    ">- `nn.init.constant`: initialisiert den Bias pro Schicht mit einem konstanten Wert 0.0.\n",
    ">- `self.encoder_Rx`: definiert die nicht-lineare Transformation der jeweiligen Schicht, welche auf die Eingabe angewandt wird: $\\sigma(\\cdot)$.\n",
    "\n",
    "Wir verwenden sog. **Leaky ReLUs**, um saturierende bzw. 'sterbende' Neuronen zu vermeiden und die Trainingskonvergenz zu beschleunigen. Die Anwendung von Leaky ReLUs ermöglicht die Berechnung von Gradienten auch innerhalb des negativen Bereichs einer Aktivierungsfunktion (siehe vorhergehendes Lab 05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_I8259HBqjt"
   },
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 256\n",
    "        self.map_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=256, bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 256, out 64\n",
    "        self.map_L2 = nn.Linear(256, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 64, out 16\n",
    "        self.map_L3 = nn.Linear(64, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 4\n",
    "        self.map_L4 = nn.Linear(16, 4, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 4, out 2\n",
    "        self.map_L5 = nn.Linear(4, 2, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_R5 = torch.nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_R5(self.map_L5(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt instanzieren wir ein Modell des Encoder Netzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the encoder network model\n",
    "encoder_train = encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend transferieren wir das Encoder Modell auf die `CPU` oder eine ggf. verfügbare `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to compute device\n",
    "encoder_train = encoder_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofern verfügbar, prüfen wir ob das Modell erfolgreich auf die `GPU`  übertragen wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die Modellstruktur visualisieren und die Netzarchitektur nochmals durch das Ausführen der folgenden Zelle überprüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhC0QTXUBqjv"
   },
   "source": [
    "### 3.2 Decoder Netzwerk Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxsh9OvXBqjw"
   },
   "source": [
    "In einem nächsten Schritt vervollständigen wir nun die Autoencoder Architektur des AAENNs durch die Implementierung des entsprechenden **Decoder Netzes $p_{\\theta}(\\hat{x}|z)$**. Der Decoder soll ebenfall aus insgesamt **fünf Schichten** von fully-connected Neuronen bestehen. Zudem soll der Decoder die Architektur des Encoders  **symmetrisch spiegeln**. Wir invertieren hierzu die Ausgestaltung der Schichten des Encoders schichtweise, gemäss der folgenden Struktur 2-4-16-64-256, im Rahmen der Implementierung des Decoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjnC8eTXBqjw"
   },
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 2, out 4\n",
    "        self.map_L1 = nn.Linear(in_features=2, out_features=4, bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.map_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.map_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.map_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 4, out 16\n",
    "        self.map_L2 = nn.Linear(4, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L2.weight)\n",
    "        nn.init.constant_(self.map_L2.bias, 0.0)\n",
    "        self.map_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 16, out 64\n",
    "        self.map_L3 = nn.Linear(16, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L3.weight)\n",
    "        nn.init.constant_(self.map_L3.bias, 0.0)\n",
    "        self.map_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 64, out 256\n",
    "        self.map_L4 = nn.Linear(64, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L4.weight)\n",
    "        nn.init.constant_(self.map_L4.bias, 0.0)\n",
    "        self.map_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 5 - in 256, out 618\n",
    "        self.map_L5 = nn.Linear(256, ori_subset_transformed.shape[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.map_L5.weight)\n",
    "        nn.init.constant_(self.map_L5.bias, 0.0)\n",
    "        self.map_S5 = torch.nn.Sigmoid()\n",
    "        \n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.map_R1(self.map_L1(x))\n",
    "        x = self.map_R2(self.map_L2(x))\n",
    "        x = self.map_R3(self.map_L3(x))\n",
    "        x = self.map_R4(self.map_L4(x))\n",
    "        x = self.map_S5(self.map_L5(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxpHMkmnBqjw"
   },
   "source": [
    "Wir instanzieren nun auch das Decoder Modell für das `CPU` bzw. `GPU`Training und überzeugen uns davon, dass das Modell erfolgreich initialisiert wurde. Hierzu visualisieren wir wieder die Netzarchitektur durch das Ausführen der nachfolgenden Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "dfME7m7fBqjw",
    "outputId": "df5e22ac-7eeb-43a2-e489-552db3f82d6a"
   },
   "outputs": [],
   "source": [
    "# intstantiate the decoder network model\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push model to compute device\n",
    "decoder_train = decoder_train.to(device)\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "962HA4IbBqjx"
   },
   "source": [
    "### 3.2 Discriminator Netzwerk Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GO9ChOLBqjx"
   },
   "source": [
    "In einem finalen Schritt erweitern wir nun die Autoencoder Architektur des AAENNs um das dritte notwendige Netzwerk. Dies geschieht durch die Implementierung des **Discriminator Netzes $d_{\\theta}(z)$**. Der Discriminator soll aus insgesamt **vier Schichten** von fully-connected Neuronen bestehen. Hierbei ist zu beachten, dass die **Eingabeschicht des Discriminators** die gleiche Dimensionalität wie die **Ausgabeschicht des Encoders** aufweist. Die vier Schichten des Discriminators umfassen die nachfolgende Anzahl Neuronen pro Schicht 256-16-4-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xLZxg0hBqjx"
   },
   "outputs": [],
   "source": [
    "# implementation of the discriminator network\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        # specify first layer - in 2, out 256\n",
    "        self.discriminator_L1 = nn.Linear(2, 256, bias=True) # init linearity\n",
    "        nn.init.xavier_uniform_(self.discriminator_L1.weight) # init weights according to [9]\n",
    "        nn.init.constant_(self.discriminator_L1.bias, 0.0) # constant initialization of the bias\n",
    "        self.discriminator_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify second layer - in 256, out 16\n",
    "        self.discriminator_L2 = nn.Linear(256, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.discriminator_L2.weight)\n",
    "        nn.init.constant_(self.discriminator_L2.bias, 0.0)\n",
    "        self.discriminator_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify third layer - in 16, out 4\n",
    "        self.discriminator_L3 = nn.Linear(16, 4, bias=True)\n",
    "        nn.init.xavier_uniform_(self.discriminator_L3.weight)\n",
    "        nn.init.constant_(self.discriminator_L3.bias, 0.0)\n",
    "        self.discriminator_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify fourth layer - in 4, out 2\n",
    "        self.discriminator_L4 = nn.Linear(4, 2, bias=True)\n",
    "        nn.init.xavier_uniform_(self.discriminator_L4.weight)\n",
    "        nn.init.constant_(self.discriminator_L4.bias, 0.0)\n",
    "        self.discriminator_S4 = torch.nn.Sigmoid()\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        x = self.discriminator_R1(self.discriminator_L1(x))\n",
    "        x = self.discriminator_R2(self.discriminator_L2(x))\n",
    "        x = self.discriminator_R3(self.discriminator_L3(x))\n",
    "        x = self.discriminator_S4(self.discriminator_L4(x))\n",
    "\n",
    "        # return result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir instanzieren nun auch das Discriminator Modell für das `CPU` bzw. `GPU`Training und überzeugen uns davon, dass das Modell erfolgreich initialisiert wurde. Hierzu visualisieren wir wieder die Netzarchitektur durch das Ausführen der nachfolgenden Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the discriminator network model\n",
    "discriminator_train = discriminator()\n",
    "\n",
    "# push model to compute device\n",
    "discriminator_train = discriminator_train.to(device)\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] discriminator architecture:\\n\\n{}\\n'.format(now, discriminator_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschliessend werfen wir noch einen Blick auf die Anzahl der Modellparameter, die wir im Folgenden beabsichtigen zu trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "C2ktcdrpBqjz",
    "outputId": "e63c0a1c-7793-4e2c-cbcc-e6a88f34fd73"
   },
   "outputs": [],
   "source": [
    "# init the number of encoder model parameters\n",
    "encoder_num_params = 0\n",
    "\n",
    "# iterate over the distinct encoder parameters\n",
    "for param in encoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    encoder_num_params += param.numel()\n",
    "\n",
    "# init the number of decoder model parameters\n",
    "decoder_num_params = 0\n",
    "    \n",
    "# iterate over the distinct decoder parameters\n",
    "for param in decoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    decoder_num_params += param.numel()\n",
    "\n",
    "# init the number of discriminator model parameters\n",
    "discriminator_num_params = 0\n",
    "    \n",
    "# iterate over the distinct discriminator parameters\n",
    "for param in discriminator_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    discriminator_num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] number of to be trained AAENN model parameters: {}.'.format(now, encoder_num_params + decoder_num_params + discriminator_num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, unser AAENN Modell umfasst eine beachtliche Gesamtzahl von **357.634 zu trainierenden Modellparametern**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jVSyWCVBqjz"
   },
   "source": [
    "### 3.4 Adversarial Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igoc5tVzBqjz"
   },
   "source": [
    "Nachdem wir nun das AAENN implementiert haben, möchten wir in einem nächsten Schritt das Netz trainieren. Bevor wir jedoch mit dem Training beginnen, ist es notwendig geeignete Fehlerfunktionen zu definieren. Zur Erinnerung: Wir wollen das Modell jeweils in zwei Trainingsphasen trainieren, nämlich (1) einer **Rekonstruktionsphase** und (2) einer **Regularisierungsphase**. Im Folgenden werden wir die Trainingsparameter für jede der Beiden Trainingsphasen festlegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LY6l5Ie3Bqjz"
   },
   "source": [
    "#### 3.4.1 Parametrisierung der Rekonstruktionsphase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-nBtzeBqjz"
   },
   "source": [
    "In der Rekonstruktionsphase wird der Encoder $q_{\\theta}(z|x)$ trainiert, um eine aggregierte Posterior-Verteilung $q(z)$ der Journalbuchungen $X$ innerhalb des latenten Raums $Z$ zu lernen. Dabei entspricht die gelernte Posterior-Verteilung komprimierten Repräsentationen der Merkmale der Journalbuchungen. In Analogie zu klassischen Autoencoder Neuronalen Netzen nutzt der Decoder $p_{\\theta}(\\hat{x}|z)$ die gelernten latenten Repräsentationen $z_{i} \\in Z$, um die Journaleinträge so getreu wie möglich zu rekonstruieren $\\hat{X}$ und den Rekonstruktionsfehler der AAENNs zu minimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhuwT1qcBqj0"
   },
   "source": [
    "Um dieses Lernziel zu erreichen, verwenden wir im folgenden unterschiedliche Rekonstruktionsfehler entsprechend der Art der zu rekonstruierenden Buchungsmerkmale. Für kategorische Merkmalsausprägungen $x^{i}_{cat}$, z.B. verwenden wir den **Binary-Cross-Entropy (BCE)** Rekonstruktionsfehler. Für numerische Merkmalsausprägungen $x^{i}_{con}$, z.B. , verwenden wir den **Mean-Squared-Error (MSE)** Rekonstruktionsfehler. Abschliessend werden pro zu rekonstruierender Journalbuchung $x^{i}$ beide Fehler additiv, wie nachfolgend, miteinander verknüpft:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gih4aR78Bqj0"
   },
   "source": [
    "<center> $\\mathcal{L}_{\\theta}^{REC}(x;\\hat{x}) = \\gamma \\hspace{1mm} \\mathcal{L}^{BCE}_{\\theta}(x_{cat};\\hat{x}_{cat}) + (1 - \\gamma) \\hspace{1mm} \\mathcal{L}^{MSE}_{\\theta}(x_{con};\\hat{x}_{con})$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCYwmGUOBqj0"
   },
   "source": [
    "wobei $x^{i}$, $i=1,...,n$ die Menge an Buchungen bezeichnet, $\\hat{x}^{i}$ die jeweiligen Rekonstruktionen und $j=1,...,k$ die verschiedenen Buchungsattribute indexiert. Nachfolgend instanzieren wir die entsprechende BCE Fehlerfunktion $\\mathcal{L}^{BCE}$ und MSE Fehlerfunktion $\\mathcal{L}^{MSE}$ der `PyTorch` Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLZkO1D1Bqj0"
   },
   "outputs": [],
   "source": [
    "# define the bce optimization criterion / loss function\n",
    "reconstruction_loss_categorical = nn.BCELoss(reduction='mean')\n",
    "\n",
    "# define the mse optimization criterion / loss function\n",
    "reconstruction_loss_numerical = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend transferieren wir die Berechnung der beiden Fehlerfunktionen auf die `CPU` oder eine ggf. verfügbare `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the optimization criterion / loss function to compute device \n",
    "reconstruction_loss_categorical = reconstruction_loss_categorical.to(device)\n",
    "reconstruction_loss_numerical = reconstruction_loss_numerical.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnljTPUbBqj1"
   },
   "source": [
    "Auf der Grundlage der Fehlerhöhe eines Mini-Batches an Buchungen berechnet die `PyTorch` Bibliothek automatisiert die Gradienten. Anschliessend werden AAENN-Parameter $\\theta$ auf Grundlage der ermittelten Gradienten optimiert. Hierzu ist es notwendig das gewünschte Optimierungsverfahren in **PyTorch** zu definieren. In der nachfolgenden Notebook Zelle verwenden wir das sog. **Adam Optimierungsverfahren** für die Optimierung der Modellparameter $\\theta$. Darüber hinaus definieren eine Lernrate $l = 0.001$ sowohl für die Optimierung des Encoder als auch das Decoder Netzwerk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ7dte_JBqj1"
   },
   "outputs": [],
   "source": [
    "# set encoder and decoder learning rate\n",
    "learning_rate_encoder = 1e-3\n",
    "learning_rate_decoder = 1e-3\n",
    "\n",
    "# define encoder and decoder optimization strategy\n",
    "encoder_optimizer = optim.Adam(encoder_train.parameters(), lr=learning_rate_encoder)\n",
    "decoder_optimizer = optim.Adam(decoder_train.parameters(), lr=learning_rate_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YnBKo9EBqj1"
   },
   "source": [
    "#### 3.4.2 Parametrisierung der Regulierungsphase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0TJJ6_cBqj1"
   },
   "source": [
    "In der Regularisierungsphase wird ein adversarial Trainingsverfahren angewandt, in welchem der Encoder $q_{\\theta}(z|x)$ des AAENN als Generator fungiert. Zusätzlich wird ein Diskriminator-Netzwerk $d_{\\phi}(z)$ auf den gelernten latenten Raum $Z$ aufgesetzt. In Analogie zum Training von Generative Adversarial Networks (GANs) wird der Discriminator des AAENN trainiert. Die Zielsetzung des Trainining ist es hierbei Samples einer **Fake Prior-Verteilung $p(z)$** in $z_{i} \\in Z$ von der erlernten **True Posterior-Verteilung $q(z)$** zu unterscheiden. Prallel hierzu wird der Encoder mit der Zielsetzung trainiert, eine **True Posterior-Verteilung** der Art zu lernen, dass $p(z) \\approx q(z)$. Hierdurch wird dem Diskriminator maximal verwirrt vorgaukelt, dass die aus $q(z)$ gezogenen Samples aus $p(z)$ stammen und umgekehrt. Eine hierfür geeignete Fehlerfunktion findet sich im sog. **Binary-Cross-Entropy (BCE)** Klassifikationsfehler, der formal wie nachfolgend definiert ist:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L^{Dis}_{\\phi}}(y^{i};\\hat{y}^{i}) = \\frac{1}{n}\\sum_{i=1}^{n} y^{i} ln(\\hat{y}^{i}) + (1-y^{i}) ln(1-\\hat{y}^{i})$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wobei $y^{i}$ das tatsächliche Klassenlabel der Verteilung (Fake Prior oder True Posterior Verteilung) bezeichnet und $\\hat{y}^{i}$ das durch den Discriminator vorhergesagte Klassenlabel.  Nachfolgend instanzieren wir die entsprechende BCE Fehlerfunktion der `PyTorch` Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbijUiDQBqj1"
   },
   "outputs": [],
   "source": [
    "# define the bce optimization criterion / loss function\n",
    "discriminator_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend transferieren wir die Berechnung der Fehlerfunktion auf die `CPU` oder eine ggf. verfügbare `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the optimization criterion / loss function to compute device \n",
    "discriminator_loss = discriminator_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rjk_l79fBqj2"
   },
   "source": [
    "Auf der Grundlage der BCE Fehlerhöhe eines Mini-Batches von latenten Samples berechnet die `PyTorch` Bibliothek automatisiert die Gradienten. Anschliessend werden die Parameter des Discriminators $\\phi$ auf Grundlage der ermittelten Gradienten optimiert. In der nachfolgenden Notebook Zelle verwenden wir das sog. **Adam Optimierungsverfahren** für die Optimierung der $\\phi$ Parameter. Darüber hinaus definieren eine Lernrate $l = 0.00001$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6a1kOzS2Bqj3"
   },
   "outputs": [],
   "source": [
    "# set generator and discriminator learning rate\n",
    "learning_rate_discriminator = 1e-5\n",
    "\n",
    "# set generator and discriminator optimization strategy\n",
    "discriminator_optimizer = optim.Adam(discriminator_train.parameters(), lr=learning_rate_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WnBg9kaBqj5"
   },
   "source": [
    "Nachdem wir die drei Netze des aAENN-Modells erfolgreich implementiert und instanziiert haben. Nehmen wir uns Zeit, die Definition der Modelle **(1) Encoder**, **(2) Decoder** und **(3) Discriminator** sowie die Fehlerfunktionen nochmals zu überprüfen und etwaige Fragen zu besprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-Cjxa-tBqj5"
   },
   "source": [
    "### 3.5 Gaussian Mixture Prior Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IX4_QvngBqj5"
   },
   "source": [
    "In einem nächsten Schritt erstellen wir eine Prior-Verteilung, welches es uns ermöglicht im Laufe des Trainingsprozesses zu samplen. Um die durch den AAENN gelernten Repräsentationen der Journalbuchungen im latenten Raum $Z$ zu partitionieren samplen wir aus einer **Prior-Verteilung $p(z)$, die aus einem Mixture von $\\tau$ multivariaten isotropen Gauss-Verteilungen $\\mathcal{N}(\\mu,\\mathcal{I})$ besteht**. Um die Ergebnisse im Nachfolgenden visualisieren zu können samplen wir aus einer zwei dimensionalen Prior-Verteilung $\\mu \\in \\mathcal{R}^{2}$ ist. Nachfolgend möchten wir die die Journalbuchungen in insgesamt fünf generative Prozesse partitionieren. Hierzu erstellen wir eine Prior-Verteilung $p(z)$, die insgesamt $\\tau=5$ isotropen Gauss-Verteilungen umfasst. \n",
    "\n",
    "Hierzu definieren wir zunächst die nachfolgenden vier Parameter: Anzahl Gauss-Verteilungen $\\tau$, Radius der einzelnen Verteilungen $r$, Standardabweichungen der Verteilungen $\\sigma$ und die Dimension der Verteilungen $\\mathcal{R}^{d}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of gaussians\n",
    "tau = 5 \n",
    "\n",
    "# define radius of each gaussian\n",
    "radius = 0.8\n",
    "\n",
    "# define the sigma of each gaussian\n",
    "sigma = 0.01\n",
    "\n",
    "# define the dimensionality of each gaussian\n",
    "dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt, erstellen wir die gewünschte **Prior-Verteilung $p(z)$**. Hierzu samplen wir jeweils 1,000 zufällige samples von jeder Gauss-Verteilung der gewünschten Prior-Verteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMVs7_FvBqj5"
   },
   "outputs": [],
   "source": [
    "# determine x and y coordinates of the target mixture of gaussians\n",
    "x_centroid = (radius * np.sin(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "y_centroid = (radius * np.cos(np.linspace(0, 2 * np.pi, tau, endpoint=False)) + 1) / 2\n",
    "\n",
    "# determine each gaussians mean (centroid) and standard deviation\n",
    "mu_gauss = np.vstack([x_centroid, y_centroid]).T\n",
    "\n",
    "# determine the number of samples to be created per gaussian\n",
    "samples_per_gaussian = 100000\n",
    "\n",
    "# iterate over the number of distinct gaussians\n",
    "for i, mu in enumerate(mu_gauss):\n",
    "\n",
    "    # case: first gaussian distribution\n",
    "    if i == 0:\n",
    "\n",
    "        # randomly sample from gaussian distribution \n",
    "        z_continous_samples_all = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "\n",
    "    # case: non-first gaussian distribution\n",
    "    else:\n",
    "\n",
    "        # randomly sample from gaussian distribution\n",
    "        z_continous_samples = np.random.normal(mu, sigma, size=(samples_per_gaussian, dim))\n",
    "\n",
    "        # collect and stack new samples\n",
    "        z_continous_samples_all = np.vstack([z_continous_samples_all, z_continous_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fT0CgtXSBqj7"
   },
   "source": [
    "Abschliessend möchten wir nun die gewünschte Prior-Verteilung $p(z)$ visuell prüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "FozilkQNBqj8",
    "outputId": "f44fbd9b-d585-408a-943a-33745de73bd6"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot prior distribution scatter plot\n",
    "ax.scatter(z_continous_samples_all[:, 0], z_continous_samples_all[:, 1], c='C0', marker=\"o\", edgecolors='w', linewidth=0.5) \n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Prior Distribution $p(z)$', size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aE1w_stCBqj8"
   },
   "source": [
    "Ok, die Samples der gewünschten Prior-Verteilung $p(z)$ entsprechen der erwarteten Verteilung. Anschliessend werden wir die Verteilung nun im Rahmen des AAENN Trainings verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAvFBlyeBqj8"
   },
   "source": [
    "## 4. Adversarial Autoencoder Neural Network Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2KNFyqFBqj8"
   },
   "source": [
    "In diesem Abschnitt möchten wir nun ein AAENN-Modell anhand der kodierten Transaktionsdaten trainieren. Darüber hinaus werfen wir einen detaillierten Blick auf die einzelnen Trainingshyperparameter und Trainingsschritte sowie den Trainingsfortschritt im Zeitverlauf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_NsNMmlBqj8"
   },
   "source": [
    "### 4.1 Definition der Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginnen wir nun damit, ein AAENN Modell für **5 Trainingsepochen** und **128 Buchungen pro Mini-Batch** zu trainieren. Diese Konfiguration der Hyperparameter bedeutet, dass der gesamte Datensatz dem AAENN ingesamt fünf Mal in Mini-Batches von 128 jeweils Buchungen zugeführt wird. Diese Hyperparameter Konfiguration hat zu Folge, dass pro Trainingsepoche **4,165 Updates** (533,009 Buchungen modulo 128 Buchungen pro Mini-Batch) der AAENN Modellparameter erfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VXom63JBqj9"
   },
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 5 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während der Trainingsphase sollen dem AENN Modell kontinuierlich Mini-Batches der gesamten Population von Buchungen zugeführt werden. Hierzu verwenden wir die `DataLoader` Funktionalität der `PyTorch` Bibliothek. Dabei handelt es sich im sog. Iteratoren, welche die Buchungen kontinuierlich in Form von Mini-Batches zur Verfügung stellen. Durch die Aktivierung des Parameters `shuffle=True` werden die Daten in jeder Epoche neu gemischt, bevor sie dem Netz zugeführt werden. In der nachfolgenden Zelle instanzieren wir einen PyTorch Dataloader der Buchungsdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNeHachtBqj9"
   },
   "outputs": [],
   "source": [
    "# convert pre-processed transactional data to PyTorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# push pre-processed transactional data to compute device\n",
    "torch_dataset = torch_dataset.to(device)\n",
    "\n",
    "# init the dataloader\n",
    "dataloader_train = dataloader.DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fpSkBadBqj-"
   },
   "source": [
    "### 4.2 Training des Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XebdkhVBqj-"
   },
   "source": [
    "In einenm nächsten Schritt starten wir nun das Modelltraining. Für jeden zugeführten Mini-Batch erfolgt der Trainingsprozess in zwei Phasen, der **(1) Rekonstruktionsphase** und der **(2) Regularisierungsphase**. Innerhalb der Rekonstruktionsphase durchläuft das Training die nachfolgenden Schritte: \n",
    "\n",
    ">1. Durchführung des Forward-pass durch das Encoder- und Decoder-Net.\n",
    ">2. Berechnen des Rekonstruktionsfehlers $\\mathcal{L^{Rec}_{\\theta}}(x^{i};\\hat{x}^{i})$.\n",
    ">3. Durchführung des Backwardpass durch das Decoder- und Encoder-Netz.\n",
    ">4. Update der Encoder $q_\\theta(\\cdot)$- und Decoder $p_\\theta(\\cdot)$ Parameter.\n",
    "\n",
    "Innerhalb der nachfolgende Regulierungsphase wird zunächst das Discriminator-Netz anhand der nachfolgenden Schritte trainiert: \n",
    "\n",
    ">1. Durchführung des forward pass durch das Generator-Netz (bzw. Encoder-Netz) um die **fake** Repräsentation $z^{i}_{fake} \\sim q_\\theta(\\cdot)$ zu erhalten.\n",
    ">2. Sampling einer **true** Repräsentation aus der gewünschten Prior-Verteilung $z^{i}_{real} \\sim p(\\cdot)$.\n",
    ">3. Berechnen des Diskriminationsfehlers $\\mathcal{L}_{\\theta}^{Dis}(z^{i}_{fake}; z^{i}_{real})$.\n",
    ">4. Update der Discriminator-Netz $d_\\phi(\\cdot)$ Parameter. \n",
    "\n",
    "Anschliessend erfolgt, innerhalb der Regularisierungsphase, das Training des Generator-Netzes (bzw. Encoder Netzes) anhand der nachfolgenden Schritte: \n",
    "\n",
    ">1. Durchführung des forward pass durch das Generator-Netz (bzw. Encoder-Netz) um die **fake** Repräsentation $z^{i}_{fake} \\sim q_\\theta(\\cdot)$ zu erhalten.\n",
    ">2. Berechnen des Diskriminationsfehlers $\\mathcal{L}_{\\theta}^{Dis}(z^{i}_{fake}; z^{i}_{real})$.\n",
    ">3. Update der Generator-Netz (bzw. Encoder-Netz) $q_\\theta(\\cdot)$ Parameter.\n",
    "\n",
    "Um das Lernen während des Trainings AAENN-Modells zu gewährleisten, werden wir anschliessend beobachten, ob die unterschiedlichen Netzwerkfehler mit fortschreitendem Training abnehmen. Pro durchgeführter Trainingsepoche ermitteln wir deshalb kontinuierlich den Rekonstruktionsfehler $\\mathcal{L}_{\\theta}^{Rec}$, den Diskriminationsfehler $\\mathcal{L}_{\\theta}^{DIS}$ und den Generationsfehler $\\mathcal{L}_{\\theta}^{GEN}$. Die Analyse der Fehler ermöglicht es uns zu evaluieren, ob die verschiedenen Netze lernen bzw. die Fehler konvergieren.\n",
    "\n",
    "Nach jeder abgeschlossenen Trainingsepoche möchten wir zudem einen sog. **Modell Checkpoint** pro Netz speichern. Die Checkpoints enthalten eine Bestandsaufnahme bzw. 'Schnappschuss' der Modellparameter. Im Allgemeinen ist es eine gute Praxis, während des Trainings solche Checkpoints in regelmässigen Abständen zu speichen. Sollte das Training einmal unterbrochen werden, kann es beginnend auf dem letzten Checkpoint wieder fortgesetzt werden. Für das Speichern eines Modell Checkpoints verwenden wir die nachfolgende `PyTorch` Anweisung:\n",
    "\n",
    ">- `torch.save()`: speichert den Checkpoint der aktuellen Modellparameterwerte auf dem lokalen Dateisystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6usKHnkBqkA"
   },
   "source": [
    "Anschliessend möchten das Training eines AAENN Modells beginnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "nq5Ba-2bBqkA",
    "outputId": "9dd806be-7b21-4edb-c884-04537c6fd593"
   },
   "outputs": [],
   "source": [
    "# init collection of training losses\n",
    "epoch_reconstruction_losses = []\n",
    "epoch_discriminator_losses = []\n",
    "epoch_generator_losses = []\n",
    "\n",
    "# initialize training adversarial autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "    \n",
    "    # init epoch training losses\n",
    "    batch_reconstruction_losses = 0.0\n",
    "    batch_discriminator_losses = 0.0\n",
    "    batch_generator_losses = 0.0\n",
    "\n",
    "    # set networks in training mode\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "    discriminator_train.train()\n",
    "    \n",
    "    # start timer\n",
    "    start_time = dt.datetime.now()\n",
    "\n",
    "    # iterate over epoch mini batches\n",
    "    for mini_batch_data in dataloader_train:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "  \n",
    "        # push mini batch data to compute device\n",
    "        mini_batch_data = mini_batch_data.to(device)\n",
    "        \n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== reconstruction phase =====================\n",
    "        \n",
    "        # run autoencoder encoding - decoding\n",
    "        z_sample = encoder_train(mini_batch_data)\n",
    "        mini_batch_reconstruction = decoder_train(z_sample)\n",
    "\n",
    "        # split input mini-batch data into numerical and categorical part\n",
    "        batch_cat = mini_batch_data[:, :ori_dataset_cat_processed.shape[1]]\n",
    "        batch_num = mini_batch_data[:, ori_dataset_num_processed.shape[1]:]\n",
    "        \n",
    "        # split reconstructed mini-batch data intoi numerical and categorical part\n",
    "        rec_batch_cat = mini_batch_reconstruction[:, :ori_dataset_cat_processed.shape[1]]\n",
    "        rec_batch_num = mini_batch_reconstruction[:, ori_dataset_num_processed.shape[1]:]\n",
    "\n",
    "        # determine categorical and numerical reconstruction loss\n",
    "        rec_error_categorical = reconstruction_loss_categorical(input=rec_batch_cat, target=batch_cat)  # one-hot attr error\n",
    "        rec_error_numerical = reconstruction_loss_numerical(input=rec_batch_num, target=batch_num)  # numeric attr error\n",
    "\n",
    "        # determine combined reconstruction loss\n",
    "        reconstruction_loss_all = rec_error_categorical + rec_error_numerical\n",
    "        \n",
    "        # run backward pass - determine gradients\n",
    "        reconstruction_loss_all.backward()\n",
    "        \n",
    "        # collect batch reconstruction loss\n",
    "        batch_reconstruction_losses += reconstruction_loss_all.item()\n",
    "        \n",
    "        # update network parameter - decoder and encoder\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== discriminator training ===================\n",
    "\n",
    "        # set discriminator in evaluation mode\n",
    "        discriminator_train.eval()\n",
    "\n",
    "        # generate true latent space data\n",
    "        z_true_batch = z_continous_samples_all[rd.sample(range(0, z_continous_samples_all.shape[0]), mini_batch_size),:]\n",
    "\n",
    "        # convert to torch tensor and push to compute device\n",
    "        z_true_batch = torch.FloatTensor(z_true_batch).to(device)\n",
    "\n",
    "        # determine mini batch sample generated by the encoder -> fake gaussian sample\n",
    "        z_fake_gauss = encoder_train(mini_batch_data)\n",
    "\n",
    "        # determine discriminator classification of both samples\n",
    "        d_true_gauss = discriminator_train(z_true_batch) # true sampled gaussian \n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss) # fake created gaussian\n",
    "\n",
    "        # create discriminator label of true and fake sample \n",
    "        d_true_gauss_target = torch.ones(d_true_gauss.shape).to(device) # true -> 1\n",
    "        d_fake_gauss_target = torch.zeros(d_fake_gauss.shape).to(device) # fake -> 0\n",
    "\n",
    "        # determine individual discrimination losses\n",
    "        discriminator_loss_true = discriminator_loss(target=d_true_gauss_target, input=d_true_gauss) # true loss\n",
    "        discriminator_loss_fake = discriminator_loss(target=d_fake_gauss_target, input=d_fake_gauss) # fake loss\n",
    "        \n",
    "        # combine true loss and fake loss\n",
    "        discriminator_loss_all = discriminator_loss_fake + discriminator_loss_true\n",
    "\n",
    "        # run backward through the discriminator network\n",
    "        discriminator_loss_all.backward()\n",
    "        \n",
    "        # collect discriminator loss\n",
    "        batch_discriminator_losses += discriminator_loss_all.item()\n",
    "\n",
    "        # update network the discriminator network parameters\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "        # =================== generator training =======================\n",
    "\n",
    "        # set encoder / generator in training mode\n",
    "        encoder_train.train()\n",
    "        \n",
    "        # reset the encoder / generator networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "\n",
    "        # determine fake gaussian sample generated by the encoder / generator\n",
    "        z_fake_gauss = encoder_train(mini_batch_data)\n",
    "\n",
    "        # determine discriminator classification of fake gaussian sample\n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss)\n",
    "\n",
    "        # determine discriminator classification target variables\n",
    "        d_fake_gauss_target = torch.FloatTensor(torch.ones(d_fake_gauss.shape)).to(device) # fake -> 1\n",
    "\n",
    "        # determine discrimination loss of fake gaussian sample\n",
    "        generator_loss = discriminator_loss(target=d_fake_gauss_target, input=d_fake_gauss)\n",
    "        \n",
    "        # collect generator loss\n",
    "        batch_generator_losses += generator_loss.item()\n",
    "\n",
    "        # run backward pass - determine gradients\n",
    "        generator_loss.backward()\n",
    "\n",
    "        # update network paramaters - encoder / generatorc\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "    # collect epoch training losses - reconstruction loss\n",
    "    epoch_reconstruction_loss = batch_reconstruction_losses / mini_batch_count\n",
    "    epoch_reconstruction_losses.extend([epoch_reconstruction_loss])\n",
    "    \n",
    "    # collect epoch training losses - discriminator loss\n",
    "    epoch_discriminator_loss = batch_discriminator_losses / mini_batch_count\n",
    "    epoch_discriminator_losses.extend([epoch_discriminator_loss])\n",
    "    \n",
    "    # collect epoch training losses - generator loss\n",
    "    epoch_generator_loss = batch_generator_losses / mini_batch_count\n",
    "    epoch_generator_losses.extend([epoch_generator_loss])\n",
    "    \n",
    "    # print epoch reconstruction loss\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, reconstruction loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_reconstruction_loss))\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, discriminator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_discriminator_loss))\n",
    "    print('[LOG TRAIN {}] epoch: {:04}/{:04}, generator loss: {:.4f}'.format(now, epoch + 1, num_epochs, epoch_generator_loss))\n",
    "    \n",
    "    # =================== save model snapshots to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(models_directory, encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(models_directory, decoder_model_name))\n",
    "    \n",
    "    # save trained discriminator model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_discriminator_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(discriminator_train.state_dict(), os.path.join(models_directory, decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnHdNC0nBqkA"
   },
   "source": [
    "In einem nächsten Schritt visualisieren wir die jeweiligen Rekonstruktionsfehler bzw. Diskriminationsfehler für jede Trainingsepoche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "wkrpELqHBqkA",
    "outputId": "3b8a71cf-5278-4c0f-95a4-da55b3557bb9"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the reconstruction loss per training epoch\n",
    "plt.plot(range(1, len(epoch_reconstruction_losses)+1), epoch_reconstruction_losses)\n",
    "\n",
    "# set plot axis labels\n",
    "plt.xlabel('Training Epoch', size=10)\n",
    "plt.ylabel('Reconstruction Loss', size=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('AAENN Model Training', size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "e4g9UCw1BqkB",
    "outputId": "6d595937-549b-47a5-d132-68f43d4e148d"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the discriminator loss per training epoch\n",
    "plt.plot(range(0, len(epoch_discriminator_losses)), epoch_discriminator_losses)\n",
    "\n",
    "# set plot axis labels\n",
    "plt.xlabel('Training Epoch', size=10)\n",
    "plt.ylabel('Discrimination Loss', size=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('AAENN Model Training', size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "xpbcRem9BqkB",
    "outputId": "7510517d-b132-4048-dd8a-1c6c92d8194b"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the generator loss per training epoch\n",
    "plt.plot(range(0, len(epoch_generator_losses)), epoch_generator_losses)\n",
    "\n",
    "# set plot axis labels\n",
    "plt.xlabel('Training Epoch', size=10)\n",
    "plt.ylabel('Generation Loss', size=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('AAENN Model Training', size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kv6lhcx8BqkC"
   },
   "source": [
    "Nach den ersten fünf Trainingsepochen ist zu beobachten, dass der **Rekonstruktionsfehler kontinuierlich abnihmmt**. Dies deutet darauf hin, dass es dem Adversarial Autoencoder zunehment gelingt die Journalbuchungen korrekt zu rekonstruieren.\n",
    "\n",
    "Aus den Schaubildern wird auch deutlich, dass das Modelltraining noch nicht konvergiert. D.h. das AAENN Modell könnte noch einige weitere Epochen trainiert werden. Um im Rahmen des Labs jedoch etwas Zeit zu sparen, möchten wir das Notebook nun mit einem bereits für 400 Epochen vortrainierten Modell fortsetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoQOZj-rBqkC"
   },
   "source": [
    "## 5. Evaluating the Autoencoder Neural Network (AENN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ha6OVXjTBqkC"
   },
   "source": [
    "In diesem Abschnitt möchten wir die Fähigkeit des erlernten AAENN Modells zur Erkennung von Anomalien in Buchhaltungsdaten evaluieren. Hierzu werden wir auf vortrainierte AENN Modelle zurück greifen. Die Evaluation umfasst die **lokalen** als auch die **globalen** Anomalien des Datensatzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Laden der Modell Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54rLvmZXBqkC"
   },
   "source": [
    "Für die Evaluation laden wir üblicherweise das AAENN-Modell mit **geringstem Rekonstruktions- bzw. Diskriminationsfehler**. Pro Trainingsepoche wurde im Rahmen des Modelltrainings jeweils ein Checkpoint der Modellparameter innerhalb des lokalen Modellverzeichnis gespeichert. Wir werden nun die bereits für **400 Trainingsepochen** trainierten Modell Checkpoint laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pv35N4kEBqkC",
    "outputId": "c6f0c4ac-5a22-432c-e4b5-820988b0cb90"
   },
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = 'https://raw.githubusercontent.com/GitiHubi/CADS/main/lab_06/03_models/20220818-03_22_18_ep_401_encoder_model.pth'\n",
    "decoder_model_name = 'https://raw.githubusercontent.com/GitiHubi/CADS/main/lab_06/03_models/20220818-03_22_18_ep_401_decoder_model.pth'\n",
    "\n",
    "# Read stored model from the remote location\n",
    "encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "decoder_bytes = urllib.request.urlopen(decoder_model_name)\n",
    "\n",
    "# Load tensor from io.BytesIO object\n",
    "encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "decoder_buffer = io.BytesIO(decoder_bytes.read())\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder().to(device)\n",
    "decoder_eval = decoder().to(device)\n",
    "    \n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_buffer, map_location=lambda storage, loc: storage))\n",
    "decoder_eval.load_state_dict(torch.load(decoder_buffer, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BycUlpLBqkD"
   },
   "source": [
    "### 5.2 Visualisierung des Latenten Raums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpXoyzWrBqkD"
   },
   "source": [
    "In einem nächsten Schritt, möchten wir nun den latenten Raum $Z$ inkl. der erlernten Repräsentationen $z_{i}$ des bereits vortrainierten Modells visualisieren. Hierzu definieren wir uns zunächst einen Dataloader, welcher die Journalbuchungen dem AAENN-Modell zur Verfügung stellt ohne die Buchungen zu mischen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IJRjyXqBqkD"
   },
   "outputs": [],
   "source": [
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader_eval = dataloader.DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW9cvdFwBqkE"
   },
   "source": [
    "Anschliessend berechnen wir die erlernte Repräsentation $z_{i}$ für jede Journalbuchung $z_{i}$ des Datensatzes. Hierzu führen wir einen forward pass durch das Encoder-Netz für jede Journalbuchung durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0M9EGtWBqkE"
   },
   "outputs": [],
   "source": [
    "# set networks in evaluation mode\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# init batch count\n",
    "batch_count = 0\n",
    "\n",
    "# iterate over epoch mini batches\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "\n",
    "    # determine latent space representation of all transactions\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "    \n",
    "    # case: initial batch \n",
    "    if batch_count == 0:\n",
    "\n",
    "      # collect reconstruction errors of batch\n",
    "      z_enc_transactions_all = z_enc_transactions_batch\n",
    "      \n",
    "    # case: non-initial batch\n",
    "    else:\n",
    "      \n",
    "      # collect reconstruction errors of batch\n",
    "      z_enc_transactions_all = torch.cat((z_enc_transactions_all, z_enc_transactions_batch), dim=0)\n",
    "    \n",
    "    # increase batch count\n",
    "    batch_count += 1\n",
    "\n",
    "# convert to numpy array\n",
    "z_enc_transactions_all = z_enc_transactions_all.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMGA1GTlBqkE"
   },
   "source": [
    "In einem Folgeschritt, möchten wir die erlernte Repräsentation $z_{i}$ jeder Journalbuchung $x_{i}$ innerhalb des latenten Raums $Z \\in \\mathcal{R}^{2}$ visualisieren. Visualiserung soll zudem kennzeichnen, ob es sich bei einer jeweiligen Journalbuchung um eine reguläre Buchung oder eine Anomalie handelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "K9P4oQaXBqkE",
    "outputId": "4b0193f0-ae75-4012-feb9-0ba15d9dad01"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = z_enc_transactions_all[label == 'regular']\n",
    "global_outliers = z_enc_transactions_all[label == 'global']\n",
    "local_outliers = z_enc_transactions_all[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', marker=\"o\", label='regular', edgecolors='w', linewidth=0.5) # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"x\", label='global', edgecolors='w', s=60) # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C3', marker=\"x\", label='local', edgecolors='w', s=60) # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ_v__AvBqkF"
   },
   "source": [
    "### 5.3 Berechnung der Normalisierten Mean-Divergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wsz6oQ-BqkF"
   },
   "source": [
    "Grundsätzlich lässt sich beobachten, dass Journalbuchungen die ungewöhnliche Attributwerte aufweisen (globale Anomalien, siehe Lab 05), in Regionen mit geringer Wahrscheinlichkeitsdichte des latenten Raum ihre Anordnung finden. D.h. im Falle der Prior-Verteilung dieses Notebooks, eine erhöhte Divergenz von den Means des Mixture Gaussians $\\mathcal{N}(\\mu,\\mathcal{I})$ aufweisen. Der Parameter $\\tau$ bezeichnet die Anzahl der der verschiedenen Gaussians, die durch $\\mu =\\{\\mu^1 \\ldots \\mu^\\tau \\}, t=1...\\tau$ definiert sind und $\\mu^{t} \\in \\mathcal{R}^m$. \n",
    "\n",
    "Anhand der Regulierungsphase des AAENN-Trainings werden die Repräsentationen der Journalbuchungen grds. in Richtung der Regionen mit hoher Wahrscheinlichkeitsdichte der Prior-Verteilung 'geschoben'. Zugleich werden durch die Rekonstruktionsphase, Repräsentationen die seltenen oder anomalen Journalbuchungen entsprechen in Regionen mit geringer Wahrscheinlichkeitsdichte platziert. Dies hat zur Folge, dass reguläre Journalbuchungen eine geringere Distanz zu den Means der Prior-Verteilung aufweisen. \n",
    "\n",
    "Nachfolgend ermitteln wir für jede Repräsentation $z^{i}$ den euklidischen Abstand zu ihrem nächstgelegenen Mean $\\mu^\\tau$. Formal kann die Mean-Divergenz kann anhand der nachfolgenden Formel berechnet werden $D_{\\theta^*}^{\\tau}(z^{i};\\mu) = \\min\\limits_{\\tau} \\lVert z^i-\\mu^\\tau \\rVert^2$. Für die Ermittlung eines Anomaly-Scores globaler Anomalien berechnen wir die normalisierte Mean-Divergenz $MD$ unter gegebenen (optimalen) Modellparametern $\\theta^*$, ausgedrückt durch:\n",
    "\n",
    "\\begin{equation}\n",
    "MD_{\\theta^*}^{\\tau}(x^{i}) = \\frac{D_{\\theta^*}^{\\tau}(z^i;\\mu) - D_{\\theta^*, min}^{\\tau}}{D_{\\theta^*, max}^{\\tau} - D_{\\theta^*, min}^{\\tau}},\n",
    "\\end{equation}\n",
    "\n",
    "wobei $D_{min}$ und $D_{max}$ die Minimal- und Maximalwerte des erhaltenen Mean-Divergenz bezeichnen, welche durch $\\mathcal{L}_{\\theta^*}$ und den jeweils nächsten Modus $\\tau$ der Buchung determiniert werden. Für die Berechnung der Mean-Divergenz implementieren wir zunächst die **Euklidische Distanz** zwischen zwei Punkten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35iFX_etBqkF"
   },
   "outputs": [],
   "source": [
    "# define euclidean distance calculation\n",
    "def compute_euclid_distance(x, y):\n",
    "    \n",
    "    # calculate euclidean distance \n",
    "    euclidean_distance = np.sqrt(np.sum((x - y) ** 2, axis=1))\n",
    "    \n",
    "    # return euclidean distance\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGx1sDrABqkF"
   },
   "source": [
    "Anschliessend berechnen wir die Mean-Divergence $D_{\\theta}^{\\tau}$ der Repräsentationen $z_{i}$ der Journalbuchungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd_LQqFsBqkG"
   },
   "outputs": [],
   "source": [
    "# determine distance to each mode\n",
    "distances = np.apply_along_axis(func1d=compute_euclid_distance, axis=1, arr=z_enc_transactions_all, y=mu_gauss)\n",
    "\n",
    "# determine mode divergence\n",
    "mean_divergence = np.min(distances, axis=1)\n",
    "\n",
    "# determine min-mode id\n",
    "cluster_ids = np.argmin(distances, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxEPbeTfBqkG"
   },
   "source": [
    "In einem nächsten Schritt, normalisieren wir die erhaltene Mean-Divergence pro Journalbuchung $D_{\\theta^*}^{\\tau}$ anhand der die Minimal- und Maximalwerte $D_{min}$ und $D_{max}$ des jeweils zugehörigen Means $\\tau$ der Prior-Verteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNOsg4lsBqkG"
   },
   "outputs": [],
   "source": [
    "# prepare empty arrays of the same shape and dtype\n",
    "mean_divergence_all_scaled = np.asarray(mean_divergence)\n",
    "\n",
    "# iterate over the cluster means\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "  \n",
    "    # determine journal entries of current mode\n",
    "    mask = cluster_ids == cluster_id\n",
    "\n",
    "    # normalize journal entries mean divergence to the range [0,1]\n",
    "    mean_divergence_all_scaled[mask] = (mean_divergence[mask] - mean_divergence[mask].min()) / (mean_divergence[mask].ptp())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEisc853BqkH"
   },
   "source": [
    "Abschliessend visualisieren wir die normalisierte Mean-Divergence $MD_{\\theta^*}^{\\tau}$ der Repräsentationen $z^{i}$ der Journalbuchungen $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "xNxfJuQTBqkH",
    "outputId": "92d04476-405b-412a-80e3-248e230a9148"
   },
   "outputs": [],
   "source": [
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(mean_divergence_all_scaled, name='mean_divergence'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "# determine number of clusters\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    # sample data fraction for faster visualization\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    \n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "    \n",
    "    # seperate regular and anomalous journal entries\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['mean_divergence'], c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['mean_divergence'], c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['mean_divergence'], c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=20)\n",
    "\n",
    "    # set y-axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    # set x-axis ticks and labels\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('Mean-Divergence $MD$', fontsize=20)\n",
    "\n",
    "# init plot legend\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "\n",
    "# plot legend\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0., bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid plot layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h27jv8E9BqkH"
   },
   "source": [
    "### 5.4 Berechnung des Normalisierten Rekonstruktionsfehlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n89gRgosBqkH"
   },
   "source": [
    "Grundsätzlich lässt sich beobachten, dass Journalbuchungen die ungewöhnliche Korrelationen der Attributwerte aufweisen (lokale Anomalien, siehe Lab 05), in einem hohen Rekonstruktionsfehler resultieren. Dies resultiert aus der Rekonstruktionsphase des AAENN-Architektur. Durch die verlustbehaftete Kompression des Encoders werden ungewöhnliche bzw. Anomale Attributskorrelationen nicht durch das AAENN Model gelernt. Dies resultiert zudem in einer Überlagerung von regulären Buchungen und lokalen Anomalien im latenten Raum. \n",
    "\n",
    "Nachfolgend ermitteln wir für jede Journalbuchung $x^{i}$ den kombinierten Rekonstruktionsfehler $\\mathcal{L}^{Rec}$ bestehend aus MSE und BCE Fehler (siehe Definition oben). Für jede Journalbuchung wird anschliessend der normalisierte Rekonstruktionsfehler $\\mathcal{L}^{Rec}$ unter gegebenen (optimalen) Modellparametern $\\theta^*$ berechnet, ausgedrückt durch:\n",
    "\n",
    "\\begin{equation}\n",
    "RE_{\\theta^*}^{\\tau}(x^{i};\\hat{x}^{i}) = \\frac{\\mathcal{L}_{\\theta^*}^{Rec, \\tau}(x^i;\\hat{x}^{i}) - \\mathcal{L}_{\\theta^*, min}^{Rec, \\tau}}{\\mathcal{L}_{\\theta^*, max}^{Rec, \\tau} - \\mathcal{L}_{\\theta^*, min}^{Rec, \\tau}},\n",
    "\\end{equation}\n",
    "\n",
    "wobei $\\mathcal{L}_{min}^{Rec}$ und $\\mathcal{L}_{max}^{Rec}$ die Minimal- und Maximalwerte der erhaltenen Rekonstruktionsfehler bezeichnen, welche durch $\\mathcal{L}_{\\theta^*}$ und den jeweils nächsten Modus $\\tau$ der Buchung determiniert werden. Nachfolgend ermitteln wir zunächst den Rekonstruktionsfehler $\\mathcal{L}_{\\theta^*}^{Rec, \\tau}$ für jeden Journaleintrag $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsEtBljsBqkH"
   },
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "reconstruction_criterion_categorical_eval = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "reconstruction_criterion_numeric_eval = nn.MSELoss(reduction='none').to(device)\n",
    "\n",
    "# set networks in evaluation mode\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# init batch count\n",
    "batch_count = 0\n",
    "\n",
    "# iterate over epoch mini batches\n",
    "for enc_transactions_batch in dataloader_eval:\n",
    "\n",
    "    # determine latent space representation of all transactions\n",
    "    z_enc_transactions_batch = encoder_eval(enc_transactions_batch)\n",
    "\n",
    "    # reconstruct input samples\n",
    "    reconstruction_batch = decoder_eval(z_enc_transactions_batch)\n",
    "\n",
    "    # split input transactions into numeric and categorical parts\n",
    "    input_cat_all = enc_transactions_batch[:, :ori_dataset_cat_processed.shape[1]]\n",
    "    input_num_all = enc_transactions_batch[:, ori_dataset_num_processed.shape[1]:]\n",
    "\n",
    "    # split reconstruction into numeric and categorical parts\n",
    "    rec_cat_all = reconstruction_batch[:, :ori_dataset_cat_processed.shape[1]]\n",
    "    rec_num_all = reconstruction_batch[:, ori_dataset_num_processed.shape[1]:]\n",
    "\n",
    "    # compute rec error\n",
    "    rec_error_cat_all = reconstruction_criterion_categorical_eval(input=rec_cat_all, target=input_cat_all).mean(dim=1)\n",
    "    rec_error_num_all = reconstruction_criterion_numeric_eval(input=rec_num_all, target=input_num_all).mean(dim=1)\n",
    "\n",
    "    # combine categorical and numerical errors\n",
    "    rec_error_all_batch = rec_error_cat_all + rec_error_num_all\n",
    "    \n",
    "    # case: initial batch\n",
    "    if batch_count == 0:\n",
    "    \n",
    "      # collect reconstruction errors of batch\n",
    "      rec_error_all = rec_error_all_batch\n",
    "    \n",
    "    # case: non-initial batch\n",
    "    else:\n",
    "      \n",
    "      # collect reconstruction errors of batch\n",
    "      rec_error_all = torch.cat((rec_error_all, rec_error_all_batch), dim=0)\n",
    "    \n",
    "    # increase batch count\n",
    "    batch_count += 1\n",
    "\n",
    "# convert to numpy array\n",
    "rec_error_all = rec_error_all.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgGyJtnSBqkI"
   },
   "source": [
    "In einem nächsten Schritt, normalisieren wir den erhaltenen Rekonstruktionsfehler pro Journalbuchungen $\\mathcal{L}^{Rec}$ anhand der Minimal- bzw. Maximalwerte der erhaltenen Rekonstruktionsfehler $\\mathcal{L}_{min}^{Rec}$ bzw. $\\mathcal{L}_{max}^{Rec}$ pro zugehörigem Mean $\\tau$ der Prior-Verteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6OXNSRTBqkI"
   },
   "outputs": [],
   "source": [
    "# prepare empty arrays of the same shape and dtype\n",
    "rec_error_all_scaled = np.asarray(rec_error_all)\n",
    "\n",
    "# iterate over the cluster modes\n",
    "for cluster_id in np.unique(cluster_ids).tolist():\n",
    "  \n",
    "    # determine journal entries of current mode\n",
    "    mask = cluster_ids == cluster_id\n",
    "\n",
    "    # normalize mode journal entries reconstruction error to the range [0,1]\n",
    "    rec_error_all_scaled[mask] = (rec_error_all[mask] - rec_error_all[mask].min()) / (rec_error_all[mask].ptp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oxpMUtjBqkI"
   },
   "source": [
    "Abschliessend visualisieren wir den normalisierten Rekonstruktionsfehler $RE_{\\theta^*}^{\\tau}$ der Journalbuchungen $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "v25TKzLEBqkI",
    "outputId": "2010716d-2fb6-433e-aafa-d8a438e8a10e"
   },
   "outputs": [],
   "source": [
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(rec_error_all_scaled, name='rec_error'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "# determine number of clusters\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    # sample data fraction for faster visualization\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    \n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    # seperate regular and anomalous journal entries\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['rec_error'], c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['rec_error'], c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['rec_error'], c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=20)\n",
    "\n",
    "    # set y-axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    # set x-axis ticks and labels\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('Reconstruction Error $RE$', fontsize=20)\n",
    "\n",
    "# init plot legend\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "\n",
    "# plot legend\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0., bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid plot layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKOEWs9uBqkJ"
   },
   "source": [
    "### 5.4 Berechnung des Anomaly Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XzRvnHWBqkJ"
   },
   "source": [
    "Die Mean-Divergenz und der Rekonstruktionsfehler einer Journalbuchung ermöglichen den Rückschluss, ob es sich bei der Buchung um eine gewöhnliche Buchung handelt bzw. ob die Buchung aus einer regulären Geschäftsaktivität resultiert. Um **globale** und **lokale Anomalien** zu ermitteln, soll nachfolgend jede Journalbuchung $x^i$ anhand der normalisierten Mean-Divergenz $MD_{\\theta^*}^{\\tau}$ und des Rekonstruktionsfehlers $RE_{\\theta^*}^{\\tau}$ bewertet werden. Hierzu verwenden wir den nachfolgenden Anomaly Score (AS), welcher beide Kriterien miteinander verknüpft:\n",
    "\n",
    "\\begin{equation}\n",
    "AS^{\\tau}(x^{i};\\hat{x}^{i}) = \\alpha \\times RE_{\\theta^*}^{\\tau}(x^{i};\\hat{x}^{i}) + (1-\\alpha) \\times MD_{\\theta^*}^{\\tau}(x^{i}),\n",
    "\\end{equation} \n",
    "\n",
    "wobei $x^{i}$ eine Journalbuchnug bezeichnet, $\\theta^*$ ein gegebenes Set an Modellparametern und $\\tau$ die Anzahl der Gaussians der Prior-Verteilung. Wir verknüpfen beide Eigenschaften additv und verwenden den Faktor $\\alpha$ um beide Eigenschaften zu gewichten. Nachfolgend berechnen den Score pro Journalbuchung mit $\\alpha=0.4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-0DJy2SBqkJ"
   },
   "outputs": [],
   "source": [
    "# set alpha \n",
    "alpha = 0.4\n",
    "\n",
    "# determine journal entry anomaly score\n",
    "anomaly_score = alpha * rec_error_all_scaled + (1.0 - alpha) * mean_divergence_all_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPhizQIFBqkL"
   },
   "source": [
    "Abschliessend visualisieren wir den Anomaly Score $AS^{\\tau}$ pro Journalbuchung $x^{i}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "QwFmh4f1BqkL",
    "outputId": "f9085ecb-de33-41cd-9c67-adf16bd0da58"
   },
   "outputs": [],
   "source": [
    "# collect anomaly score, labels and cluster assignments\n",
    "plot_data = pd.concat([pd.Series(anomaly_score, name='anomaly_score'), \n",
    "                       pd.Series(label, name='label'),                        \n",
    "                       pd.Series(cluster_ids, name='cluster_id')],\n",
    "                     axis=1)\n",
    "\n",
    "# determine number of clusters\n",
    "num_clusters = len(np.unique(cluster_ids))\n",
    "\n",
    "# init sub-plots based on the number of modes\n",
    "fig, axes = plt.subplots(1, num_clusters, sharey=True, figsize=(14, 10))\n",
    "\n",
    "# iterate over distinct modes\n",
    "for mode in range(0, num_clusters):\n",
    "\n",
    "    # sample data fraction for faster visualization\n",
    "    plot_data = plot_data.sample(frac=1.0)\n",
    "    \n",
    "    # collect features of current mode\n",
    "    z_mode = plot_data[plot_data['cluster_id'] == mode]\n",
    "\n",
    "    # seperate regular and anomalous journal entries\n",
    "    regular_data = z_mode[z_mode['label'] == 'regular']\n",
    "    global_outliers = z_mode[z_mode['label'] == 'global']\n",
    "    local_outliers = z_mode[z_mode['label'] == 'local']\n",
    "\n",
    "    # create train scatter plot of regular samples\n",
    "    axes[mode].scatter(regular_data.index, regular_data['anomaly_score'], c='C0', marker='o', s=30, linewidth=0.3, label='regular', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of global anomalies\n",
    "    axes[mode].scatter(global_outliers.index, global_outliers['anomaly_score'], c='C1', marker='x', s=120, linewidth=3, label='global', edgecolors='w')\n",
    "    \n",
    "    # create train scatter plot of local anomalies\n",
    "    axes[mode].scatter(local_outliers.index, local_outliers['anomaly_score'], c='C3', marker='x', s=120, linewidth=3, label='local', edgecolors='w')\n",
    "\n",
    "    # set axis labels\n",
    "    xlabel = '$\\\\tau={}$' + str(mode+1) if mode == 0 else str(mode+1)\n",
    "    axes[mode].set_xlabel(xlabel, fontsize=20)\n",
    "\n",
    "    # set y-axis limits\n",
    "    axes[mode].set_ylim([0.0, 1.1])\n",
    "\n",
    "    # set x-axis ticks and labels\n",
    "    axes[mode].set_xticks([int(plot_data.shape[0]/2)])\n",
    "    axes[mode].set_xticklabels(['$x_{i}$'])\n",
    "\n",
    "# set axis labels\n",
    "axes[0].set_ylabel('Anomaly Score $AS$', fontsize=20)\n",
    "\n",
    "# init plot legend\n",
    "handles, labels = axes[2].get_legend_handles_labels()\n",
    "\n",
    "# plot legend\n",
    "plt.legend(handles, labels, loc='center', fontsize=20, ncol=3, borderaxespad=0., bbox_to_anchor=(-6.5, 1., 9., .1))\n",
    "\n",
    "# set grid plot layout\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYqRpiYnBqkL"
   },
   "source": [
    "Die Visualisierung zeigt, dass die regulären Journalbuchungen im Allgemeinen einen niedrigen Anomaly Score aufweisen. Zugleich weisen die im Buchungsstoff enthaltenen Anomalien einen vergleichsweise hohen Anomaly Score auf. Infolgedessen wird deutlich, dass sich anhand des Scorings 'globale' (orange) als auch 'lokale' (grün) Anomalien von regulären Tagebucheinträgen (blau) unterscheiden lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset['label'] = label\n",
    "ori_dataset['tau'] = cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um beispielhaft einige der ermittelten Anomalien zu untersuchen, extrahieren wir nachfolgend die Journalbuchungen mit einem Anomaly Score >= 0.25 des Gaussian-Means $\\tau=3$. Hierzu werden wir in einem ersten Schritt einen Zeitstempel des Datenextrakts für den Audit-Trail der Prüfung generieren: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend extrahieren wir die gefilterten **globalen Anomalien** als Excel-Datei zur weiteren substantiellen Prüfung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2TpJytaRBqkL",
    "outputId": "16fe9b35-dae6-4e08-9533-ce1e6fbf8cb1"
   },
   "outputs": [],
   "source": [
    "# inspect transactions exhibiting a reconstruction error >= 0.2\n",
    "autoencoder_global_anomalies = ori_dataset[(anomaly_score >= 0.25) & (cluster_ids == 2)]\n",
    "\n",
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - AAE_001_autoencoder_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_global_anomalies.to_excel(data_directory, header=True, index=False, sheet_name=\"Global_Anomalies\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cj7GovE_BqkM"
   },
   "source": [
    "Darüber hinaus möchten wir beispielhaft weitere Anomalien untersuchen. Hierzu extrahieren wir die Journalbuchungen mit einem Anomaly-Score >= 0.4 des Gaussian-Means $\\tau=2$. Hierzu erstellen wir in einem ersten Schritt wieder einen Zeitstempel für den Audit-Trail der Prüfung: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "dmKQmznCBqkM",
    "outputId": "baa2f46a-65b7-44eb-f044-0aa5debd0663"
   },
   "outputs": [],
   "source": [
    "# inspect transactions exhibiting a anomaly_score >= 0.4 from the mode 2\n",
    "autoencoder_local_anomalies = ori_dataset[(anomaly_score >= 0.4) & (cluster_ids == 1)]\n",
    "\n",
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - AAE_002_autoencoder_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_local_anomalies.to_excel(data_directory, header=True, index=False, sheet_name=\"Local_Anomalies\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Aufgaben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Ihr wissen zu vertiefen empfehlen wir, die nachfolgenden Übungen zu bearbeiten:\n",
    "\n",
    "**1. Trainieren und evaluieren Sie ein Adversarial Autoencoder Neural Network Model mit erhöhter Anzahl Means des Priors.**\n",
    "\n",
    "> Die innerhalb des Notebooks vorgestellte Architektur führte zu einem guten Modell für die Erkennung von Anomalien. Prüfen Sie wie sich die Performance verändert, wenn Sie die Anzahl der Means der Mixture of Gaussian Prior-Verteilung erhöhen. Wie ändert sich die Fähigkeit des Modells Anomalien im Buchungsstoff zu erkennen? Lassen sich unterschiedliche Aussagen für globale bzw. lokale Anomalien treffen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Evaluieren Sie unterschiedliche Gewichtungen $\\alpha$ des Anomaly Scores.**\n",
    "\n",
    "> Die innerhalb des Notebooks vorgestellte Gewichtung der normalisierten Mean-Divergence und des normalisierten Rekonstruktionsfehlers führte zu einem guten Ergebnis für die Erkennung von Anomalien. Prüfen Sie wie sich die Performance verändert, wenn Sie die Gewichtung der beiden Einflussfaktoren abänderen. Wie ändert sich die Fähigkeit des Modells Anomalien im Buchungsstoff zu erkennen? Lassen sich unterschiedliche Aussagen für globale bzw. lokale Anomalien treffen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h9s5Awk7BqkM",
    "outputId": "5173ae1e-19c1-4669-b388-58f2fc66293d"
   },
   "source": [
    "## Lab Zusammenfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Notebook umfasste eine schrittweise Einführung in **Entwurf, Implementierung, Training und Bewertung** eines auf einem neuronalen Netz basierenden Ansatzes zur Erkennung von Anomalien in Buchhaltungsdaten. Ein besonderer Augenmerk wurde hierbei auf das adversarial Lernen des Autoencoder Netzes gelegt. Die vorgestellten Code Beispiele und die Übungen können als Ausgangspunkt für für die Entwicklung und das Testen komplexerer Strategien zur Erkennung von Anomalien dienen."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "colab_06.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
