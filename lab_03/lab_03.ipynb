{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"./fs_logo.png\">\n",
    "\n",
    "##  Lab 03 - Unsupervised Machine Learning\n",
    "\n",
    "Seminar Künstliche Intelligenz, Frankfurt School, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analysen des Seminars **Künstliche Intelligenz** des Zertifikatstudiengangs **Certified Audit Data Scientist (CADS)** basieren auf Jupyter Notebook. Anhand solcher Notebooks ist es möglich eine Vielzahl von Datenanalysen und statistischen Validierungen durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"./lab_03_banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Lab haben Sie die verschiedenen Elemente eines Supervised Machine Learning Workflow kennengelernt z.B. Datenaufbereitung, Model Training und Validierung. In diesem dritten Lab werden wir unseren ersten **Unsupervised Machine Learning** Workflow mit dem in der Vorlesung vorgestellten **k Means-Clustering)** Algorithmus erstellen. Wir werden dieses Verfahren für das Clustern von nicht gelabelten Daten (d.h. von Daten ohne definierte Kategorien oder Gruppen) verwenden. \n",
    "\n",
    "Der **k-Means-Clustering**-Algorithmus ist einer der beliebtesten Clustering-Algorithmen des maschinellen Lernen's. Die Zielsetzung des k-Means Clustering ist es, Cluster (Gruppen) in einem gegebenen Datensatz zu finden. Das Verfahren kann dazu verwendet werden Hypothesen über die innerhalb eines Datensatzes **vorhandenen Gruppen zu verfizieren**. Darüber hinaus findet das Verfahren oftmals Anwendung bei der **Identifizierung unbekannter Gruppen** in komplexen Datensätzen. Einige Beispiele für geschäftsbezogene Anwendungsfälle sind:\n",
    "\n",
    ">- Segmentierung von Kunden nach Kaufhistorie.\n",
    ">- Segmentierung von Benutzern nach Verhalten auf eine Webseite.\n",
    ">- Gruppieren von Beständen nach Verkaufsaktivitäten.\n",
    ">- Gruppieren von Beständen nach Fertigungsmetriken.\n",
    "\n",
    "(Quelle: https://www.datascience.com/blog/k-means-clustering)\n",
    "\n",
    "Wir werden den **k Means-Clustering** Algorithmus dazu verwenden, um zu lernen, Beobachtungen des **Iris Datensatzes** zu gruppieren. Die folgende Abbildung zeigt einen Überblick über den Prozess des maschinellen Lernens, welchen wir in diesem Notebook erstellen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"./splash.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei etwaigen Fragen wenden Sie sich, wie immer gerne an uns via **marco (dot) schreyer (at) unisg (dot) ch**. Wir wünschen Ihnen Viel Freude mit unseren Notebooks und Ihren revisorischen Analysen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lernziele des Labs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der heutigen Übung sollten Sie in der Lage sein:\n",
    "\n",
    "> 1. Den **Unsupervised Machine Learning** Workflow in Form eines Notebook zu erstellen.\n",
    "> 2. Den **k-Means Clustering** Algorithmus zu trainieren und zu evaluieren. \n",
    "> 3. Die Python **sklearn Bibliothek** zu verwenden, um beliebige Clustering Verfahren zu trainieren bzw. zu optimieren.\n",
    "> 4. Die nahezu **optimale Anzahl von Clustern** für einen unbekannten Datensatz zu ermitteln. \n",
    "> 5. Die **gewonnenen Ergebnisse** eines Clustering auszuwerten und zu interpretieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einrichten der Analyseumgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie in den vorangegangenen Übungen müssen wir eine Reihe von Python-Bibliotheken importieren, die Datenanalyse und -visualisierung ermöglichen. In dieser Übung werden wir die Bibliotheken `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` und `Seaborn` verwenden. Nachfolgend importieren wir die benötigten Bibliotheken durch die Ausführung der folgenden Anweisungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "import warnings\n",
    "\n",
    "# import the pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import the scipy spatial distance capability\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import sklearn k-means classifier library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import matplotlibs 3D plotting capabilities\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausschalten möglicher Warnmeldungen z.B. aufgrund von zukünftigen Änderungen der Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktivieren der sog. Inline-Darstellung von Visualisierungen in Jupyter-Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwenden des `Seaborn` Visualisierungstil's in allen nachfolgenden Visualisierungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Festlegen eines zufälligen Seeds zur Gewährleistung der Reproduzierbarkeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datenakquise und Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datensatz Download und Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **Iris-Datensatz** ist ein klassischer und einfacher Datensatz, der oft als \"Hello World\"-Beispiel in der Mehrklassen-Klassifikation verwendet wird. Dieser Datensatz besteht aus Messungen von drei verschiedenen Arten von Irisblüten (als **Klassen** bezeichnet), nämlich der Iris Setosa, der Iris Versicolour und der Iris Virginica, und ihrer jeweiligen gemessenen Blütenblatt- und Kelchblattlänge (als **Merkmale** bezeichnet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"iris_dataset.png\">\n",
    "\n",
    "(Quelle: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insgesamt besteht der Datensatz aus **150 Samples** (50 Samples pro Klasse) sowie den entsprechenden **4 verschiedenen Features**, die für jede Probe durchgeführt wurden. Nachfolgend, die Liste der einzelnen Features:\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details of the dataset can be obtained from the following puplication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Laden wir nun den Datensatz und führen wir ein erstes Assessment der erhaltenen Daten durch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Prüfen der Feature-Dimensionalität des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Prüfen der Feature-Dimensionalität des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Prüfen der Label-Dimensionalität des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Prüfen der im Datensatz enthaltenen Klassen Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stellen wir uns kurz vor, wie die Features des Datensatzes extrahiert wurden und innerhalb des Datensatzes dargestellt werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"featurecollection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Inspektion der ersten fünf Featurezeilen des Iris-Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe und Inspektion der korrespondierenden Labels der ersten fünf Featurezeilen des Iris-Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden, führen wir nun eine eingehendere Bewertung der Daten durch. Dazu visualisieren wir die Merkmalsverteilungen des Iris-Datensatzes entsprechend ihrer jeweiligen Klassenzugehörigkeit sowie die paarweisen Beziehungen der Merkmale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden die Python-Bibliothek **Seaborn**, um eine solche Visualisierung zu erstellen, die auch als **Pairplot** bezeichnet wird. Die Seaborn Bibliothek ist eine leistungsstarke Datenvisualisierungsbibliothek, die auf der Matplotlib basiert. Sie bietet eine hervorragende Schnittstelle zum Zeichnen informativer statistischer Grafiken (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# supervised scenario\n",
    "# sns.pairplot(iris_plot, diag_kind='hist', hue='species');\n",
    "\n",
    "# unsupervised scenario\n",
    "sns.pairplot(iris_plot, diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem erstellten Pairplot ist ersichtlich, dass die meisten Features, die der Blütenklasse `setosa` entsprechen, eine **lineare Trennbarkeit** von den Features der übrigen Blütenklassen aufweisen. Darüber hinaus weisen die Blütenklassen `versicolor` und `virginica` eine **nicht lineare Trennbarkeit** über alle gemessenen Features des Iris-Datensatzes auf.\n",
    "\n",
    "Stellen wir uns nun vor, dass wir das Label, welches mit jeder Beobachtung im Iris-Datensatz verbunden ist, nicht wissen. Wie könnten wir die innerhalb des Datensatzes vorhandenen drei Klassen dennoch unterscheiden bzw. ableiten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgRYuUMKt8rL"
   },
   "source": [
    "### 2.3 Feature Skalierung des Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVzLw8uot8rL"
   },
   "source": [
    "Bei Betrachtung der Featurewerte des **Iris-Datensatzes** wird augenscheinlich, dass die jeweiligen Wertebereiche stark variieren. Dies stellt eine Herausforderung für abstandsbasierte Algorithmen wie den **k Means-Clustering** Algorithmus dar. Solche Algorithmen berechnen den Abstand zwischen zwei Samples anhand eines Abstandsmaßes wie z.B. dem **Euklidischen** oder **Manhattan** Abstand.\n",
    "\n",
    "Weist eines dieser Merkmale einen großen Wertebereich auf, dominiert der berechnete Abstand dieses speziellen Features die Funktion des Algorithmnus. Deshalb ist es notwendig den Wertebereich der verschiedenen Features zu skalieren, z.B. auf einen Wertebereich zwischen $[0,1]$ oder $[-1,1]$. Hierdurch ist gewährleistet, dass jedes Feature gleichwertig zum Ergebnis des Algorithmus beiträgt. Ein verbreitetes Verfahren zur Skalierung von Merkmalen wird als **Min-Max-Normalisierung** bezeichnet und ist durch folgende Formel definiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6jo00Alt8rL"
   },
   "source": [
    "$$x'={\\frac  {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bo6ERPyUt8rL"
   },
   "source": [
    "Skalieren wir nun die einzelnen Featurewerte des **Iris Datensatzes** anhand der **Min-Max-Normalisierung** unter Verwendung der `MinMaxScaler` Funktionalität der `sklearn` Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccNkX14Vt8rM"
   },
   "outputs": [],
   "source": [
    "# init the min-max scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "\n",
    "# min-max normalize the distinct feature values\n",
    "iris_data_norm = scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsWFNsVTt8rS"
   },
   "source": [
    "Ausgabe und Prüfen der ersten 5 Merkmalszeilen des normalisierten Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "1cqcjpJZt8rT",
    "outputId": "be595c68-b074-41ee-f57a-d1846b3f63d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris_data_norm, columns=iris.feature_names).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3GbEr0jt8rX"
   },
   "source": [
    "Wir können nun feststellen, dass die Merkmalswerte min-max skaliert wurden. Lassen Sie uns diese Beobachtung nun auch kurz statistisch validieren, d.h. um zu prüfen ob tatsächlich alle Merkmalswerte auf einen Wertebereich zwischen $[0,1]$ skaliert wurden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "MzFsjejPt8rX",
    "outputId": "20aebe50-91d7-4c7f-8cf4-30a2340558d2"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris_data_norm, columns=iris.feature_names).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHwTTkWxt8rY"
   },
   "source": [
    "Das Ergebnis schaut gut aus, d.h. alle Merkmalswerte liegen tatsächlich in einem Bereich zwischen $[0,1]$. Nachfolgend möchten wir das Ergebnis der neu skalierten Features auch einmal visuell analysieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "colab_type": "code",
    "id": "UGDK8Me3t8rZ",
    "outputId": "3af54c21-3275-41d9-f7b2-5484669fd9aa"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# prepare the dataset to be plotable using seaborn\n",
    "\n",
    "# convert to Panda's DataFrame\n",
    "iris_plot = pd.DataFrame(iris_data_norm, columns=iris.feature_names)\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7PF3yj1t8ra"
   },
   "source": [
    "Wunderbar, die verschiedenen Verteilungen der Merkmale blieben unverändert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k Means-Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **k-Means Clustering** Algorithmus ist einer der am häufigsten verwendeten unüberwachten Clustering-Algorithmen, um Cluster (Gruppen) in mehrdimensionalen Daten zu finden. Im Ergebnis liefert das Verfahren für jedes unbekannte Sample $x$ eine Clusterzugehörigkeit.\n",
    "\n",
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"kmeans_algorithm.png\">\n",
    "\n",
    "Um dieses Ziel zu erreichen geht der k-Means Clustering von den nachfolgenden beiden Annahmen aus:\n",
    "\n",
    "- Die Zuordnung eines Samples zu einem Cluster erfolgt anhand des am nächsten gelegenen Zentroids.\n",
    "- Der Zentroid (bzw. Mean) eines Clusters bezeichnet das arithmetische Mittel der zugordneten Samples.\n",
    "\n",
    "Bevor wir den k Means-Clustering Algorithmus für das Clustering eines Datensatzes $x$ des Iris-Datensatzes anwenden schauen wir uns nochmals die einzelnen Schritte des Algorithmus an. Dabei gehen wir zunächst von folgenden Annahmen aus:\n",
    "\n",
    "- Datensatz $D$ bestehend aus Samples $D = \\{x_1, x_2, x_3, ..., x_n\\} \\in \\mathcal{R}^d$. \n",
    "- Verschiedene $k$ Zentroiden (bzw. Means) $\\mu_{1}, \\mu_{2}, ..., \\mu_{k} \\in \\mathcal{R}^d$.\n",
    "- Zentroide (bzw. Means) $\\mu_{i}$ eines Clusters bilden das arithmetische Mittel der zugordneten Samples.\n",
    "\n",
    "Basierend auf diesen Annahmen ist es möglich die nachfolgenden Schritte des **k Means-Clustering** Algorithmus iterativ durchzuführen:\n",
    "\n",
    ">- **Schritt 1** - Initialisiere $k$ Zentroide $\\mu_{1}, \\mu_{2}, ..., \\mu_{k} \\in \\mathcal{R}^d$ durch eine zufällige Auswahl von $k$ Samples $x_i$.\n",
    ">- **Schritt 2** - Berechne für jedes Sample $x_i$ den Abstand zu jedem Zentroid und ordne $x_i$ dem jeweils nächstgelegenen Zentroid zu.\n",
    ">- **Schritt 3** - Berechne die neuen $k$ Zentroide $\\mu_{1}, \\mu_{2}, ..., \\mu_{k} \\in \\mathcal{R}^d$ auf Grundlage der zugordneten Samples.\n",
    ">- **Schritt 4** - Wiederhole die Schritte 2 und 3 iterativ, bis sich die Zentroide und Zuordnungen nicht mehr ändern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualisierung des Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt möchten wir den k Means-Clustering Algorithmus auf den Iris-Datensatz anwenden. Wir beginnen mit einem einführenden Beispiel auf der Grundlage der beiden Features `Petal Length (cm)` und `Petal Width (cm)`. Hierzu verschaffen wir uns zunächst einen Eindruck über die Verteilung der Samples der beiden Merkmale, indem wir sie entsprechend visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.scatter(iris_data_norm[:, 2], iris_data_norm[:, 3])\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[petal_length]\", fontsize=14)\n",
    "ax.set_ylabel(\"[petal_width]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Petal Length vs. Petal Width Feature Distribution', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den **k Means-Clustering** Algorithmus zu parametrisieren ist es zunächst notwendig die entsprechenden Hyperparameter zu definieren. Hierbei handelt es sich um Parameter, welche nicht durch das Modell im Rahmen des Training gelernt werden können.\n",
    "\n",
    "In einem ersten Schritt definieren wir die **Anzahl der Cluster $k$**, die im Rahmen des Clustering erkannt werden sollen. Hierbei folgen wir implizit der Annahme, dass jede Iris Spezies einer uns unbekannten Verteilung entspricht (jede Verteilung mit anderem Mittelwert $\\mu_1$, $\\mu_2$ und $\\mu_3$). Vor diesem Hintergrund setzen wir die Anzahl der zu erkennenden Cluster, entsprechend der Anzahl Iris Spezien, auf $k=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem zweite Schritt definieren wir die **initialen Koordinaten der Cluster Zentroide $\\mu_{i}$**. Diese stellen die Ausgangspunkte der Zentroide in der ersten Iteration des k Means-Clustering Verfahrens dar. In unserem Beispiel unserem definieren wir insgesamt 3 Koordinaten mit jeweils 2 Dimensionen. Dies vor dem Hintergrund, dass wir 3 Cluster auf der Grundlage der beiden Features `Petal Length (cm)` und `Petal Width (cm)` finden möchten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_means = np.array([[1, 3], [2, 6], [1, 7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem dritten Schritt definieren wir eine **maximale Anzahl von Iterationen**, welche der k Means-Clustering Algorithmus durchführen soll. Im Allgemeinen ist das Clustering beendet, sobald sich keine Änderungen der Clusterzuordnung mehr ergeben. Für hochdimensionale Datenbestände ist es jedoch oftmals sinnvoll, eine Obergrenze der Iterationsschritte festzulegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach erfolgreicher Definition der Hyperparameter ist es nun möglich eine Instanz des **k Means-Clustering** Algorithmus mit Hilfe der `sklearn` Bibliothek zu initialisieren. Die `sklearn` Bibliothek enthält eine ausführliche Dokumentation des Algorithmus, u.a. praktische Beispiele und Anwendungsfälle, und kann unter der nachfolgenden URL abgerufen werden:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Definition der Hyperparameter initialisieren nun den **k Means-Clustering** Algorithmus der [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=no_clusters, init=init_means, max_iter=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt, trainieren wir das Clustering auf Grundlage der Feature `Petal length (cm)` und  `Petal width (cm)` des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(iris_data_norm[:,2:4]) # note that we are using column 2 (petal length) and 3 (petal width) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach erfolgreichem Training, werfen wir nun einen Blick auf die gelernten Klassenlabels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_ # obtain the assigned cluster labels\n",
    "print(labels)           # print the cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darüber hinaus möchten wir auch einen Blick die gelernten Zentroide werfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = kmeans.cluster_centers_ # obtain the assigned cluster means \n",
    "print(means)                    # print the cluster center coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt möchten wir nun die Güte des erlernten Modell's qualitativ und auch quantitativ evaluieren. Hierzu visualisieren wir zunächst die Samples auf Grundlage der beiden Features `Petal Length (cm)` und `Petal Width (cm)` und die Positionen der Cluster Zentriode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length vs. petal width and corresponding classes\n",
    "scatter = ax.scatter(iris_data_norm[:,2], iris_data_norm[:,3], c=labels.astype(np.float), cmap=plt.cm.Set1)\n",
    "\n",
    "# prepare data legend\n",
    "legend = ax.legend(*scatter.legend_elements(), loc='upper left', title='Cluster')\n",
    "\n",
    "# add legend to plot\n",
    "ax.add_artist(legend)\n",
    "\n",
    "# plot cluster means\n",
    "ax.scatter(means[:,0], means[:,1], marker='x', c='black', s=100)\n",
    "\n",
    "# iterate over distinct cluster means\n",
    "for i, mean in enumerate(means):\n",
    "    \n",
    "    # determine max cluster point distance\n",
    "    cluster_radi = cdist(iris_data_norm[:, 2:4][labels==i], [mean]).max()\n",
    "    \n",
    "    # plot cluster size\n",
    "    ax.add_patch(plt.Circle(mean, cluster_radi, fc='darkgrey', edgecolor='slategrey', lw=1, alpha=0.1, zorder=1))\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[petal_length]\", fontsize=14)\n",
    "ax.set_ylabel(\"[petal_width]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Petal Length vs. Petal Width - Clustering Results', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um eine noch besseres Intuition für den k Means-Clustering Algorithmus zu erhalten, sehen wir uns die untenstehende Animation der einzelnen Iterationen des Algorithmus an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"60%\" controls>\n",
    "<source src=\"kmeansvideo.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf Grundlage der Visualiserungen wird deutlich, dass der Algorithmus drei Cluster im Datensatz ermittelte. Prüfen wir in einem Folgeschritt nun, inwieweit das Ergebnis den tatsächlichen Klassen `virginica`, `setosa` und `versicolor` entspricht. Hierzu visualisieren wir nachfolgend die erlernte und tatsächliche Klassenzuordnung: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 6))\n",
    "\n",
    "#### plot true iris class labels\n",
    "\n",
    "# add grid\n",
    "ax[0].grid(linestyle='dotted')\n",
    "\n",
    "# iterate over distinct species\n",
    "for species in np.unique(iris.target):\n",
    "    \n",
    "    # obtain iris petal length and petal width\n",
    "    iris_features = iris_data_norm[iris.target == species,:]\n",
    "    \n",
    "    # obtain iris species name\n",
    "    iris_target_name = iris.target_names[species]\n",
    "    \n",
    "    # plot petal length vs. petal width as well as the true labels\n",
    "    ax[0].scatter(iris_features[:,2], iris_features[:,3], c='C{}'.format(str(species)), label=iris_target_name)\n",
    "\n",
    "# prepare data legend\n",
    "ax[0].legend(loc='upper left', title='Classes')\n",
    "\n",
    "# set axis range\n",
    "ax[0].set_xlim([-0.1, 1.1])\n",
    "ax[0].set_ylim([-0.1, 1.1])\n",
    "\n",
    "# add axis legends\n",
    "ax[0].set_xlabel(\"[petal_length]\", fontsize=14)\n",
    "ax[0].set_ylabel(\"[petal_width]\", fontsize=14)\n",
    "\n",
    "#### plot clustering results\n",
    "\n",
    "# add grid\n",
    "ax[1].grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length vs. petal width and corresponding classes\n",
    "scatter = ax[1].scatter(iris_data_norm[:,2], iris_data_norm[:,3], c=labels.astype(np.float), cmap=plt.cm.Set1)\n",
    "\n",
    "# prepare data legend\n",
    "ax[1].legend(*scatter.legend_elements(), loc='upper left', title='Cluster')\n",
    "\n",
    "# plot cluster means\n",
    "ax[1].scatter(means[:,0], means[:,1], marker='x', c='black', s=100)\n",
    "\n",
    "# iterate over distinct cluster means\n",
    "for i, mean in enumerate(means):\n",
    "    \n",
    "    # determine max cluster point distance\n",
    "    cluster_radi = cdist(iris_data_norm[:, 2:4][labels==i], [mean]).max()\n",
    "    \n",
    "    # plot cluster size\n",
    "    ax[1].add_patch(plt.Circle(mean, cluster_radi, fc='darkgrey', edgecolor='slategrey', lw=1, alpha=0.1, zorder=1))\n",
    "\n",
    "# set axis range\n",
    "ax[1].set_xlim([-0.1, 1.1])\n",
    "ax[1].set_ylim([-0.1, 1.1])\n",
    "    \n",
    "# add axis legends\n",
    "ax[1].set_xlabel(\"[petal_length]\", fontsize=14)\n",
    "ax[1].set_ylabel(\"[petal_width]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.suptitle('Petal Length vs. Petal Width - True Class Labels (left) and Clustering Results (right)', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine quantitative Evaluation möchten wir den Abstand aller Datensätze $x_{i}$ zu ihren nächsten Mittelwerten $\\mu_{i}$ untersuchen. Erinnern wir uns daran, dass das k-Means Clustering eine lokale Optimierung der **Residual Sum of Squares (RSS)** durchführt, die wie nachfolgend definiert ist:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n}(x_{i}-\\mu_{k(i)})^{2},$$\n",
    "\n",
    "wobei $x_{i}$ ein Sample des Datensatzes bezeichnet, und $\\mu_{k(i)}$ den entsprechend am nächsten liegenden Mittelwert innerhalb des Feature Space $\\mathcal{R}^{d}$. Nachfolgend möchten wir den RSS Wert für das vorliegende Clustering Ergebnis betrachten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = kmeans.inertia_\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Aufgaben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ihr wissen zu vertiefen empfehlen wir, die nachfolgenden Übungen zu bearbeiten:\n",
    "\n",
    "**1. Trainieren und evaluieren Sie den k Means-Clustering Algorithmus für eine unterschiedliche Anzahl max. Iterationen.**\n",
    "\n",
    "> Erhöhen Sie kontinuierlich die Anzahl der max. Iterationen $i$ des Trainings des k Means-Clustering. Beginnen sie mit einer Iteration und erhöhen Sie den Wert auf bis zu 5 Iterationen ($i=1, 2, ..., 5$). Führen Sie das Clustering pro unterschiedlichem Iterationswert durch. Was lässt sich in Bezug auf die Residual Sum of Squares (RSS) mit zunehmender Anzahl Iterationen $i$ beobachten? Bitte erläutern Sie Ihre Feststellungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Bestimmen Sie, ob der k Means-Clustering Algorithmus immer zu einem gleichen Ergebnis konvergiert.**\n",
    "\n",
    "> Überprüfen Sie sorgfältig die verschiedenen Schritte des k Means-Algorithmus. Beantworten Sie die Frage, ob der Algorithmus immer zu einem gleichen Ergebnis konvergiert? Bitte erläutern Sie Ihre Überlegungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Anwendung des k Means-Clustering Algorithmus in unterschiedlichen Datenverteilungen.**\n",
    "\n",
    "> Betrachten Sie die folgenden Datenverteilungen. Bestimmen Sie, welche sich für eine Anwendung des k Means-Clustering eignen. Bestimmen sie hierüber hinaus welcher $k$-Wert für das jeweilge Clustering verwendet werden sollte. Bitte erläutern Sie Ihre Überlegungen.\n",
    "\n",
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"clustering.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. k Means-Clustering (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualisierung des Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns nun an, wie das **k-Means Clustering** in höherdimensionalen Merkmalsräumen angewandt werden kann. Unser Ziel ist es hierbei, die Klassen des Datensatzes anhand der drei nachfolgenden Merkmale zu ermitteln `Petal length (cm)`, `Petal width (cm)` und `Sepal length (cm)`. Hierzu verschaffen wir uns zunächst einen Eindruck über die Verteilung der Samples der drei Merkmale, indem wir sie entsprechend visualisieren.\n",
    "\n",
    "Um die dreidimensionale Darstellung zu ermöglichen, laden wir die Bibliothek `matplotlib` neu und importieren ihre 3D-Darstellungsmöglichkeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from importlib import reload\n",
    "reload(plt)\n",
    "\n",
    "# import the seaborn plotting library\n",
    "import seaborn as sns\n",
    "\n",
    "# import matplotlibs 3D plotting capabilities\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verschaffen wir uns nun einen Eindruck über die Verteilung der Samples der drei Merkmale, indem wir sie entsprechend visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# init 3D plotting\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=30, azim=120)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.scatter(iris_data_norm[:,0], iris_data_norm[:,1], iris_data_norm[:,2], s=40)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[sepal_length]\", fontsize=14)\n",
    "ax.set_ylabel(\"[sepal_width]\", fontsize=14)\n",
    "ax.set_zlabel(\"[petal_length]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Sepal Length vs. Sepal Width vs. Petal Length', fontsize=14)\n",
    "\n",
    "# show the 3-dimensional plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem ersten Schritt definieren wir wieder die **Anzahl der Cluster $k$**, die im Rahmen des Clustering erkannt werden sollen. Hierbei folgen wir implizit der Annahme, dass jede Iris Spezies einer uns unbekannten Verteilung entspricht (jede Verteilung mit anderem Mittelwert $\\mu_1$, $\\mu_2$ und $\\mu_3$). Vor diesem Hintergrund setzen wir die Anzahl der zu erkennenden Cluster, entsprechend der Anzahl Iris Spezien, auf $k=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem zweite Schritt definieren wir die **initialen Koordinaten der Cluster Zentroide $\\mu_{i}$**. Diese stellen die Ausgangspunkte der Zentroide in der ersten Iteration des k Means-Clustering Verfahrens dar. In unserem Beispiel unserem definieren wir insgesamt 3 Koordinaten mit jeweils 3 Dimensionen. Dies vor dem Hintergrund, dass wir 3 Cluster auf der Grundlage der beiden Features `Sepal length (cm)`, `Sepal width (cm)`, und `Petal length (cm)` finden möchten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_means = np.array([[1.0, 3.0, 3.0], [2.0, 6.0, 5.0], [1.0, 7.0, 2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem dritten Schritt definieren wir auch wieder eine **maximale Anzahl von Iterationen**, welche der k Means-Clustering Algorithmus durchführen soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Definition der Hyperparameter initialisieren nun den **k Means-Clustering** Algorithmus der [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=no_clusters, init=init_means, max_iter=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt, trainieren wir das Clustering auf Grundlage der Feature `Sepal length (cm)`, `Sepal width (cm)`, und `Petal length (cm)` des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(iris_data_norm[:,0:3]) # note that we are using column 1 (sepal length), 2 (sepal width) and 3 (petal length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach erfolgreichem Training, werfen wir nun wieder einen Blick auf die gelernten Klassenlabels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_ # obtain the assigned cluster labels\n",
    "print(labels)           # print the cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darüber hinaus möchten wir auch einen Blick die gelernten Zentroide werfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = kmeans.cluster_centers_ # obtain the assigned cluster means \n",
    "print(means)                    # print the cluster center coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt möchten wir nun erneut die Güte des erlernten Modell's qualitativ und auch quantitativ evaluieren. Hierzu visualisieren wir zunächst die Samples auf Grundlage der beiden Features `Sepal length (cm)`, `Sepal width (cm)` und `Petal length (cm)` und die Positionen der Cluster Zentriode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# init 3D plotting\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=30, azim=120)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length vs. petal width and corresponding classes\n",
    "ax.scatter(iris_data_norm[:,0], iris_data_norm[:,1], iris_data_norm[:,2], c=labels.astype(np.float), cmap=plt.cm.Set1, s=40)\n",
    "\n",
    "# plot cluster means\n",
    "ax.scatter(means[:,0], means[:,1], means[:,2], marker='x', c='black', s=100)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[sepal_length]\", fontsize=14)\n",
    "ax.set_ylabel(\"[sepal_width]\", fontsize=14)\n",
    "ax.set_zlabel(\"[petal_length]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Sepal Length vs. Sepal Width vs. Petal Length', fontsize=14);\n",
    "\n",
    "# show the 3-dimensional plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend möchten wir, für eine quantitative Beurteilung, den RSS Wert für das vorliegende Clustering Ergebnis betrachten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = kmeans.inertia_\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Herausforderungen des Algorithmus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl der **k Means-Clustering** Algorithmus einer der beliebtesten Clustering-Algorithmen für das maschinelle Lernen darstellt hat er drei wesentliche Nachteile, die im Folgenden erläutert werden:\n",
    "\n",
    "1. Das k Means-Clustering garantiert, dass sich das Klassifikationsergebnis mit jeder Iteration verbessert. Insgesamt gibt es jedoch **keine Garantie**, dass der Algorithmus die **global besten Cluster** findet.\n",
    "\n",
    "**Praktische Lösung:** Durchführung des Algorithmus mit mehreren zufälligen Initialisierungen der Zentroide. Eine solch zufällige Initialisierung geschieht auch standardmäßig bei der Verwendung der `scikit` learn Implementierung des Algorithmus.\n",
    "\n",
    "2. Das k Means-Clustering ermöglicht nicht die Bestimmung einer **optimale Anzahl von Clustern** bzw. kann dies nicht aus den bereitgestellten Daten lernen. D.h. bei Ausführung des Algorithmus mit $k=20$ resultiert in 20 gefunden Clustern unabhängig davon wie sinnvoll diese Ergebnis ist. \n",
    "\n",
    "**Praktische Lösung:** Verwenden der sog. \"Elbow\"-Technik, wie im nächsten Abschnitt des Notebooks erläutert. Eine andere Möglichkeit besteht in der Anwendung von komplexeren Clustering Algorithmus wie z.B. Gaussian Mixture Models oder dem DBSCAN Algorithmus.\n",
    "\n",
    "3. Das k Means-Clustering resultiert im Fall **nicht linear trennbarer** Klassen innerhalb des Feature Space zu suboptimalen Ergebnissen. Dies resultiert aus der Annahme, dass Samples jeweils den nächsten liegenden Zentroid zugeordnet werden.\n",
    "\n",
    "**Praktische Lösung:** Transformation (d.h. wenn möglich) des Datensatzes in eine höhere Dimension, in der eine lineare Trennung möglich wird, z. B. durch Verwendung eines spektralen Clustering-Algorithmus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine der grundlegenden Ideen der Optimierung unüberwachter maschineller Lernverfahren, wie dem **k Means-Clustering** Algorithmus, besteht darin, eine optimale Anzahl von Cluster zu finden. Diese Zielsetzung kann durch die Analyse der **Residual Sum of Squares (RSS)** (Gesamtsumme der quadrierten Abstände) für unterschiedliche Cluster Anzahl Konfigurationen erreicht werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k^{*} =\\underset{k}{\\arg \\min} \\sum_{i=1}^{n}(x_{i}-\\mu_{k(i)})^{2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierbei bezeichnet $x_{i}$ einen einzelnen Merkmalsvektor (oder eine Beobachtung) des Datensatz und $\\mu_{k(i)}$ den nächstliegenden Mittelwert im Merkmalsraum $\\mathcal{R}^{d}$. Die Herausforderung besteht nun darin, eine optimale Anzahl Cluster $k$ für einen gegebenen Datensatz zu ermitteln? Die Auswahl des optimalen $k$ ist mit den beiden nachfolgenden assoziiert:\n",
    "\n",
    "- Die Cluster Anzahl $k$ ist zu niedrig (Untersegmentierung), dies hat sehr heterogene Cluster zur Folge.\n",
    "- Die Cluster Anzahl $k$ ist zu hoch (Übersegmentierung), dies hat sehr feinkörnige Cluster zur Folge. \n",
    "\n",
    "Beispiele für Unter. bzw. Übersegmentierung: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px; height: auto\" src=\"kselection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lösung: Verwenden der **Residual Sum of Squares (RSS)** Metrik, um eine optimale Anzahl von Clustern $k$ zu finden. Dies kann durch die Ausführung des sogenannten **'Elbow'** Verfahrens erreicht werden, die durch den folgenden Algorithmus definiert ist:\n",
    "\n",
    ">- **Schritt 1** - Berechnen des k Means-Clustering für unterschiedliche $k$ Konfigurationen.\n",
    ">- **Schritt 2** - Berechnen der RSS $k$ für die unterschiedlichen $k$ Konfigurationen.\n",
    ">- **Schritt 3** - Abtragen der RSS für jede $k$ Konfiguration in einem Schaubild. \n",
    ">- **Schritt 4** - Ermittlung des 'Elbow' der Verteilung der RSS für unterschiedliche $k$ Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem nächsten Schritt verwenden wir das **Elbow** Verfahrens um eine optimale Cluster Anzahl für den Iris Datensatz zu ermitteln. Hierzu führen wir einen Grid-Search über den Hyperparameter $k$ des k Means-Clustering Algorithmus durch. Um einen solchen Grid-Search effizient auszuführen begrenzen wir die Anzahl der Iteration pro Ausführung eines Clusterings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend führen wir den Grid-Search mit $k=1, 2, ..., 30$ Werten durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the list of squared distances\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "# define the range of k-values to investigate\n",
    "K = range(1,30)\n",
    "\n",
    "# iterate over all k-values\n",
    "for k in K:\n",
    "    \n",
    "    # init the k-Means clustering algorithm of the current k-value\n",
    "    kmeans = KMeans(n_clusters=k, init='random', max_iter=max_iterations)\n",
    "    \n",
    "    # run the k-Means clustering of sepal-length and sepal-width features\n",
    "    kmeans = kmeans.fit(iris_data_norm[:,0:2])\n",
    "    \n",
    "    # collect the sum of within squared distances of the current k-value\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Beendigung des Grid-Search wollen wir die berechneten RSS für verschieden evaluieren: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the collected sum of squared distances of each k\n",
    "sum_of_squared_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem letzten Schritt stellen wir die Verteilung der RSS für unterschiedliche $k$ Werte in einem Diagramm dar. Die Darstellung innerhalb eines solchen Diagramms ermöglicht die Bestimmung des 'Elbow' der Verteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.plot(K, sum_of_squared_distances)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[number of clusters $k$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[within-cluster distance $E$]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Cluster Number $k$ vs. Within-Cluster Distance $E$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Aufgaben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ihr wissen zu vertiefen empfehlen wir, die nachfolgenden Übungen zu bearbeiten:\n",
    "\n",
    "**1. Wenden Sie den k Means-Clustering Algorithmus auf alle vier im Iris-Datensatz enthaltenen Merkmale an.**\n",
    "\n",
    "> Wenden Sie das k Means Clustering an, um ein $k=3$ Clustering der vier im Iris-Datensatz enthaltenen Merkmale `Sepal-Länge (cm)`, `Sepal-Breite (cm)`, `Petal-Länge (cm)` und `Petal-Breite (cm)` durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Bestimmung der optimalen Cluster Anzahl $k$ bei Verwendung der der vier im Iris-Datensatz enthaltenen Merkmale.**\n",
    "\n",
    "> Bestimmen Sie eine optimale Anzahl Cluster $k$, unter Verwendung der oben beschriebenen **'Elbow'** Methodik für das Clustering des Iris Datensatzes unter Verwendung der vier im Iris-Datensatz enthaltenen Merkmale `Sepal-Länge (cm)`, `Sepal-Breite (cm)`, `Petal-Länge (cm)` und `Petal-Breite (cm)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Sie können Ihre Lösung an dieser Stelle einfügen\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Zusammenfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook umfasste eine schrittweise Einführung in einige grundlegende Konzepte eines **Unsupervised Machine Learning** Prozesses in Jupyter Notebooks. Die vorgestellten Code Beispiele und die Übungen können als Ausgangspunkt für komplexere und Ihre massgeschneiderten Analysen dienen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1035.5999755859375px",
    "left": "38px",
    "top": "109.39990234375px",
    "width": "276.79998779296875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
